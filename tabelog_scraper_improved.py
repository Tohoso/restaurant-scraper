#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ”¹è‰¯ç‰ˆé£Ÿã¹ãƒ­ã‚°ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
åº—åãƒ»é›»è©±ç•ªå·ãƒ»ä½æ‰€ã‚’æ­£ç¢ºã«å–å¾—ã™ã‚‹ãŸã‚ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼
"""

import requests
from bs4 import BeautifulSoup
import time
import re
from typing import List, Dict, Optional
import logging
from urllib.parse import urljoin, urlparse
import random

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TabelogScraperImproved:
    """æ”¹è‰¯ç‰ˆé£Ÿã¹ãƒ­ã‚°ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        self.base_url = "https://tabelog.com"
        
    def get_area_urls(self) -> Dict[str, str]:
        """åœ°åŸŸåˆ¥URLã‚’å–å¾—"""
        return {
            'æ±äº¬éƒ½': 'https://tabelog.com/tokyo/',
            'å¤§é˜ªåºœ': 'https://tabelog.com/osaka/',
            'ç¥å¥ˆå·çœŒ': 'https://tabelog.com/kanagawa/',
            'æ„›çŸ¥çœŒ': 'https://tabelog.com/aichi/',
            'ç¦å²¡çœŒ': 'https://tabelog.com/fukuoka/',
            'åŒ—æµ·é“': 'https://tabelog.com/hokkaido/',
            'äº¬éƒ½åºœ': 'https://tabelog.com/kyoto/',
            'å…µåº«çœŒ': 'https://tabelog.com/hyogo/',
            'åŸ¼ç‰çœŒ': 'https://tabelog.com/saitama/',
            'åƒè‘‰çœŒ': 'https://tabelog.com/chiba/'
        }
    
    def scrape_restaurant_list(self, area_url: str, max_pages: int = 5) -> List[str]:
        """
        ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³ãƒªã‚¹ãƒˆãƒšãƒ¼ã‚¸ã‹ã‚‰å€‹åˆ¥åº—èˆ—URLã‚’å–å¾—
        
        Args:
            area_url (str): åœ°åŸŸURL
            max_pages (int): æœ€å¤§ãƒšãƒ¼ã‚¸æ•°
            
        Returns:
            List[str]: åº—èˆ—URL ãƒªã‚¹ãƒˆ
        """
        restaurant_urls = []
        
        for page in range(1, max_pages + 1):
            try:
                # ãƒšãƒ¼ã‚¸URLæ§‹ç¯‰
                if page == 1:
                    page_url = area_url
                else:
                    page_url = f"{area_url}rstLst/{page}/"
                
                logger.info(f"ãƒšãƒ¼ã‚¸å–å¾—ä¸­: {page_url}")
                
                # ãƒšãƒ¼ã‚¸å–å¾—
                response = self.session.get(page_url, timeout=30)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # åº—èˆ—ãƒªãƒ³ã‚¯ã‚’æŠ½å‡ºï¼ˆè¤‡æ•°ã®ã‚»ãƒ¬ã‚¯ã‚¿ã‚’è©¦ã™ï¼‰
                restaurant_links = []
                
                # ã‚»ãƒ¬ã‚¯ã‚¿ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦ã™
                selectors = [
                    'a.list-rst__rst-name-target',
                    'a[href*="/A"][href*="/A"][href*="/"]',
                    '.list-rst__rst-name a',
                    '.rst-name a'
                ]
                
                for selector in selectors:
                    links = soup.select(selector)
                    if links:
                        restaurant_links = links
                        break
                
                # hrefãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã‚‚æ¤œç´¢
                if not restaurant_links:
                    all_links = soup.find_all('a', href=True)
                    restaurant_links = [link for link in all_links 
                                     if re.match(r'/[^/]+/A\d+/A\d+/\d+/', link.get('href', ''))]
                
                page_urls = []
                for link in restaurant_links:
                    href = link.get('href')
                    if href:
                        full_url = urljoin(self.base_url, href)
                        if full_url not in restaurant_urls:
                            restaurant_urls.append(full_url)
                            page_urls.append(full_url)
                
                logger.info(f"ãƒšãƒ¼ã‚¸ {page}: {len(page_urls)}ä»¶ã®åº—èˆ—URLå–å¾—")
                
                if not page_urls:
                    logger.info(f"ãƒšãƒ¼ã‚¸ {page} ã§åº—èˆ—URLãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                    break
                
                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œ
                time.sleep(random.uniform(1, 3))
                
            except Exception as e:
                logger.error(f"ãƒšãƒ¼ã‚¸ {page} ã®å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        logger.info(f"åˆè¨ˆ {len(restaurant_urls)} ä»¶ã®åº—èˆ—URLå–å¾—")
        return restaurant_urls
    
    def scrape_restaurant_detail(self, restaurant_url: str) -> Optional[Dict]:
        """
        å€‹åˆ¥åº—èˆ—ãƒšãƒ¼ã‚¸ã‹ã‚‰è©³ç´°æƒ…å ±ã‚’å–å¾—
        
        Args:
            restaurant_url (str): åº—èˆ—URL
            
        Returns:
            Optional[Dict]: åº—èˆ—æƒ…å ±
        """
        try:
            logger.info(f"åº—èˆ—è©³ç´°å–å¾—ä¸­: {restaurant_url}")
            
            # ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã‚’å–å¾—
            response = self.session.get(restaurant_url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # åº—åå–å¾—
            shop_name = self._extract_shop_name(soup)
            
            # é›»è©±ç•ªå·ã¨ä½æ‰€ã¯åœ°å›³ãƒšãƒ¼ã‚¸ã‹ã‚‰å–å¾—
            map_url = restaurant_url.rstrip('/') + '/dtlmap/'
            phone, address = self._extract_contact_info_from_map(map_url)
            
            # ãã®ä»–ã®æƒ…å ±ã‚’å–å¾—
            genre = self._extract_genre(soup)
            station = self._extract_station(soup)
            open_time = self._extract_open_time(soup)
            
            # å¿…é ˆé …ç›®ãƒã‚§ãƒƒã‚¯
            if not shop_name:
                logger.warning(f"åº—åãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ: {restaurant_url}")
                return None
            
            restaurant_info = {
                'shop_name': shop_name,
                'phone': phone,
                'address': address,
                'genre': genre,
                'station': station,
                'open_time': open_time,
                'url': restaurant_url,
                'source': 'é£Ÿã¹ãƒ­ã‚°'
            }
            
            logger.info(f"âœ… åº—èˆ—æƒ…å ±å–å¾—æˆåŠŸ: {shop_name}")
            return restaurant_info
            
        except Exception as e:
            logger.error(f"åº—èˆ—è©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼ {restaurant_url}: {e}")
            return None
    
    def _extract_shop_name(self, soup: BeautifulSoup) -> str:
        """åº—åã‚’æŠ½å‡º"""
        name_selectors = [
            'h1.display-name',
            '.rst-name',
            'h1',
            '.shop-name',
            'title'
        ]
        
        for selector in name_selectors:
            name_elem = soup.select_one(selector)
            if name_elem:
                name_text = name_elem.get_text(strip=True)
                # ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰åº—åéƒ¨åˆ†ã‚’æŠ½å‡º
                if ' - ' in name_text:
                    name_text = name_text.split(' - ')[0]
                if '(' in name_text:
                    name_text = name_text.split('(')[0]
                return name_text.strip()
        
        return ""
    
    def _extract_contact_info_from_map(self, map_url: str) -> tuple:
        """åœ°å›³ãƒšãƒ¼ã‚¸ã‹ã‚‰é›»è©±ç•ªå·ã¨ä½æ‰€ã‚’å–å¾—"""
        try:
            response = self.session.get(map_url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            phone = ""
            address = ""
            
            # åº—èˆ—æƒ…å ±ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰å–å¾—
            info_rows = soup.find_all('tr')
            for row in info_rows:
                th = row.find('th')
                td = row.find('td')
                
                if th and td:
                    th_text = th.get_text(strip=True)
                    td_text = td.get_text(strip=True)
                    
                    if 'äºˆç´„' in th_text or 'ãŠå•ã„åˆã‚ã›' in th_text:
                        # é›»è©±ç•ªå·ã‚’æŠ½å‡º
                        phone_match = re.search(r'[\d\-\(\)]+', td_text)
                        if phone_match:
                            phone = phone_match.group()
                    
                    elif 'ä½æ‰€' in th_text:
                        address = td_text
            
            # åˆ¥ã®ã‚»ãƒ¬ã‚¯ã‚¿ã‚‚è©¦ã™
            if not phone:
                phone_elems = soup.select('.rst-info-table__tel, .tel-wrap')
                for elem in phone_elems:
                    phone_text = elem.get_text(strip=True)
                    phone_match = re.search(r'[\d\-\(\)]+', phone_text)
                    if phone_match:
                        phone = phone_match.group()
                        break
            
            if not address:
                address_elems = soup.select('.rst-info-table__address, .address')
                for elem in address_elems:
                    address = elem.get_text(strip=True)
                    if address:
                        break
            
            return phone, address
            
        except Exception as e:
            logger.error(f"åœ°å›³ãƒšãƒ¼ã‚¸å–å¾—ã‚¨ãƒ©ãƒ¼ {map_url}: {e}")
            return "", ""
    
    def _extract_genre(self, soup: BeautifulSoup) -> str:
        """ã‚¸ãƒ£ãƒ³ãƒ«ã‚’æŠ½å‡º"""
        genre_selectors = [
            '.rst-info-table__genre',
            '.genre',
            '.category'
        ]
        
        for selector in genre_selectors:
            genre_elem = soup.select_one(selector)
            if genre_elem:
                return genre_elem.get_text(strip=True)
        
        return ""
    
    def _extract_station(self, soup: BeautifulSoup) -> str:
        """æœ€å¯„ã‚Šé§…ã‚’æŠ½å‡º"""
        station_selectors = [
            '.rst-info-table__station',
            '.station',
            '.access'
        ]
        
        for selector in station_selectors:
            station_elem = soup.select_one(selector)
            if station_elem:
                return station_elem.get_text(strip=True)
        
        return ""
    
    def _extract_open_time(self, soup: BeautifulSoup) -> str:
        """å–¶æ¥­æ™‚é–“ã‚’æŠ½å‡º"""
        time_selectors = [
            '.rst-info-table__open-time',
            '.open-time',
            '.business-hour'
        ]
        
        for selector in time_selectors:
            time_elem = soup.select_one(selector)
            if time_elem:
                return time_elem.get_text(strip=True)
        
        return ""
    
    def scrape_area_restaurants(self, area_name: str, max_restaurants: int = 100) -> List[Dict]:
        """
        åœ°åŸŸã®é£²é£Ÿåº—æƒ…å ±ã‚’å–å¾—
        
        Args:
            area_name (str): åœ°åŸŸå
            max_restaurants (int): æœ€å¤§å–å¾—ä»¶æ•°
            
        Returns:
            List[Dict]: åº—èˆ—æƒ…å ±ãƒªã‚¹ãƒˆ
        """
        area_urls = self.get_area_urls()
        
        if area_name not in area_urls:
            logger.error(f"æœªå¯¾å¿œã®åœ°åŸŸã§ã™: {area_name}")
            return []
        
        area_url = area_urls[area_name]
        
        # åº—èˆ—URLãƒªã‚¹ãƒˆå–å¾—
        max_pages = max(1, max_restaurants // 20)  # 1ãƒšãƒ¼ã‚¸ç´„20ä»¶ã¨ã—ã¦è¨ˆç®—
        restaurant_urls = self.scrape_restaurant_list(area_url, max_pages)
        
        # æœ€å¤§ä»¶æ•°ã«åˆ¶é™
        restaurant_urls = restaurant_urls[:max_restaurants]
        
        # å„åº—èˆ—ã®è©³ç´°æƒ…å ±å–å¾—
        restaurants = []
        for i, url in enumerate(restaurant_urls, 1):
            logger.info(f"é€²æ—: {i}/{len(restaurant_urls)}")
            
            restaurant_info = self.scrape_restaurant_detail(url)
            if restaurant_info:
                restaurants.append(restaurant_info)
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œ
            time.sleep(random.uniform(2, 4))
            
            # é€”ä¸­çµŒéè¡¨ç¤º
            if i % 10 == 0:
                logger.info(f"å–å¾—æ¸ˆã¿: {len(restaurants)}/{i} ä»¶")
        
        logger.info(f"åœ°åŸŸ {area_name}: {len(restaurants)} ä»¶ã®åº—èˆ—æƒ…å ±å–å¾—å®Œäº†")
        return restaurants

def test_improved_tabelog_scraper():
    """æ”¹è‰¯ç‰ˆé£Ÿã¹ãƒ­ã‚°ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®ãƒ†ã‚¹ãƒˆ"""
    scraper = TabelogScraperImproved()
    
    print("ğŸ” æ”¹è‰¯ç‰ˆé£Ÿã¹ãƒ­ã‚°ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ ãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    # æ±äº¬éƒ½ã§å°‘æ•°ã®åº—èˆ—ã‚’ãƒ†ã‚¹ãƒˆå–å¾—
    restaurants = scraper.scrape_area_restaurants('æ±äº¬éƒ½', max_restaurants=3)
    
    if restaurants:
        print(f"âœ… {len(restaurants)}ä»¶ã®åº—èˆ—æƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸ")
        
        for i, restaurant in enumerate(restaurants, 1):
            print(f"\n{i}. {restaurant['shop_name']}")
            print(f"   é›»è©±: {restaurant['phone']}")
            print(f"   ä½æ‰€: {restaurant['address']}")
            print(f"   ã‚¸ãƒ£ãƒ³ãƒ«: {restaurant['genre']}")
            print(f"   æœ€å¯„ã‚Šé§…: {restaurant['station']}")
    else:
        print("âŒ åº—èˆ—æƒ…å ±ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")

if __name__ == "__main__":
    test_improved_tabelog_scraper()

