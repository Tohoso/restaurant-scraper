# アーキテクチャドキュメント

## 概要

本プロジェクトは、食べログから飲食店情報を効率的に収集するためのWebスクレイピングシステムです。高速な非同期処理、モジュラーなアーキテクチャ、そして堅牢なエラーハンドリングを特徴としています。

## システム構成

### ディレクトリ構造

```
restaurant-scraper/
├── config/              # 設定管理
│   ├── __init__.py
│   ├── constants.py     # 定数定義
│   ├── settings.py      # 環境設定
│   └── logging_config.py # ロギング設定
├── scrapers/            # スクレイパー実装
│   ├── __init__.py
│   ├── base.py          # 基底スクレイパークラス
│   └── async_scraper.py # 非同期スクレイパー
├── utils/               # ユーティリティ
│   ├── __init__.py
│   ├── validators.py    # データバリデーション
│   ├── error_handler.py # エラーハンドリング
│   └── progress.py      # 進捗管理
├── tests/               # テストコード
│   ├── __init__.py
│   ├── test_validators.py
│   ├── test_error_handler.py
│   ├── test_base_scraper.py
│   └── test_progress.py
├── cache/               # キャッシュディレクトリ
├── results/             # 結果出力ディレクトリ
└── logs/                # ログファイル
```

## 主要コンポーネント

### 1. 設定管理 (config/)

#### constants.py
- エリアURL、セレクタ、正規表現パターンなどの定数を一元管理
- ジャンルキーワードや除外パターンの定義

#### settings.py
- 環境変数を使用した動的な設定管理
- デフォルト値とオーバーライドのサポート
- 実行時設定の集中管理

#### logging_config.py
- 統一されたロギング設定
- ログレベル、フォーマット、出力先の管理
- LoggerMixinによる簡単なロギング機能の提供

### 2. スクレイパー (scrapers/)

#### base.py - BaseTabelogScraper
基底クラスとして以下の機能を提供：
- 共通のデータ抽出メソッド
- HTTPヘッダー管理
- テキスト正規化処理
- ジャンル検証ロジック

主要メソッド：
- `_extract_shop_name()`: 店名抽出
- `_extract_phone()`: 電話番号抽出
- `_extract_address()`: 住所抽出
- `_extract_genre()`: ジャンル抽出（駅名フィルタリング付き）
- `_extract_station()`: 最寄駅抽出
- `_extract_open_time()`: 営業時間抽出

#### async_scraper.py - TabelogAsyncScraper
高速な非同期スクレイピングを実装：
- 最大50同時接続のサポート
- セマフォによる接続数制御
- バッチ処理による効率的なメモリ管理
- 進捗の永続化と再開機能

### 3. ユーティリティ (utils/)

#### validators.py
データの検証と正規化：
- `RestaurantData`: 型安全なデータクラス
- `DataValidator`: 各種バリデーションメソッド
  - 電話番号の検証と正規化
  - URLの形式チェック
  - 住所の妥当性確認
  - テキストのクリーンアップ

#### error_handler.py
堅牢なエラーハンドリング：
- カスタム例外クラス階層
- リトライデコレータ（指数バックオフ付き）
- エラー統計とログ記録
- ネットワークエラーの分類と適切な対処

#### progress.py
進捗管理と永続化：
- `ProgressMixin`: 基本的な進捗管理機能
- `BatchProgressTracker`: バッチ処理用の拡張機能
- JSONファイルによる進捗の保存/復元
- 中断からの再開サポート

## データフロー

1. **初期化**
   - 設定の読み込み
   - ロギングの設定
   - 進捗の復元（前回の続きから）

2. **URL収集**
   - エリアページから店舗リストを取得
   - ページネーション処理
   - 重複URLの除去

3. **詳細情報取得**
   - バッチ単位で非同期リクエスト
   - HTMLパースとデータ抽出
   - バリデーションと正規化

4. **エラーハンドリング**
   - ネットワークエラーの自動リトライ
   - レート制限の検出と対処
   - エラーログの記録

5. **結果出力**
   - JSON形式での中間保存
   - Excel形式での最終出力
   - データ統計の生成

## パフォーマンス最適化

### 非同期処理
- aiohttpによる非同期HTTP通信
- asyncioセマフォによる同時接続数制御
- バッチ処理によるメモリ効率の向上

### キャッシング
- 進捗の定期保存
- 処理済みURLの記録
- 中断からの効率的な再開

### レート制限対策
- ランダムな遅延の導入
- 429エラーの検出と長時間待機
- User-Agentのローテーション

## セキュリティとコンプライアンス

### robots.txtの遵守
- 適切なクロール間隔の維持
- User-Agentの明示
- 過度なアクセスの回避

### データプライバシー
- 個人情報の適切な処理
- ローカル環境での実行
- 外部サービスへの依存最小化

## 拡張性

### 新しいデータソースの追加
1. `scrapers/base.py`を継承した新しいスクレイパークラスを作成
2. 必要に応じて抽出メソッドをオーバーライド
3. `config/constants.py`に新しいセレクタを追加

### カスタムバリデーションの追加
1. `utils/validators.py`に新しいバリデーションメソッドを追加
2. `RestaurantData`クラスを拡張
3. テストケースを追加

## トラブルシューティング

### よくある問題と解決方法

1. **タイムアウトエラー**
   - `TIMEOUT`環境変数を増やす
   - `MAX_CONCURRENT`を減らして負荷を軽減

2. **レート制限エラー**
   - `DELAY_MIN`と`DELAY_MAX`を増やす
   - 同時接続数を減らす

3. **メモリ不足**
   - `BATCH_SIZE`を小さくする
   - 定期的な進捗保存を有効化

4. **データ欠損**
   - セレクタの更新が必要な可能性
   - HTMLの構造変更を確認
   - ログファイルでエラーを確認