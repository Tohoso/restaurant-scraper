#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
é£Ÿã¹ãƒ­ã‚°ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ ã‚¦ãƒ«ãƒˆãƒ©ç‰ˆ V3
- 50,000ä»¶ã®å¤§é‡åé›†å¯¾å¿œ
- ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®æœ€é©åŒ–
- ã‚¨ãƒ©ãƒ¼ã‹ã‚‰ã®è‡ªå‹•å¾©æ—§
- åœ°åŸŸã¨ã‚¸ãƒ£ãƒ³ãƒ«ã®çµ„ã¿åˆã‚ã›ã§åé›†ç¯„å›²ã‚’æ‹¡å¤§
"""

import asyncio
import aiohttp
import json
import re
import random
import time
import os
import sys
import gc
from datetime import datetime
from typing import Dict, List, Optional, Set, Tuple
from bs4 import BeautifulSoup
from pathlib import Path
import logging
from aiohttp import ClientTimeout, TCPConnector

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('tabelog_ultra_v3.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class TabelogUltraScraperV3:
    """ã‚¦ãƒ«ãƒˆãƒ©é«˜é€Ÿãƒ»å¤§é‡åé›†å¯¾å¿œé£Ÿã¹ãƒ­ã‚°ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
    
    def __init__(self, max_concurrent: int = 100):
        """åˆæœŸåŒ–"""
        self.max_concurrent = max_concurrent
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.session = None
        self.delay_range = (0.5, 1.5)  # ã‚ˆã‚ŠçŸ­ã„é…å»¶
        
        # é€²æ—ç®¡ç†
        self.processed_urls: Set[str] = set()
        self.results: List[Dict] = []
        self.errors_count = 0
        self.consecutive_errors = 0
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        self.cache_dir = Path("cache")
        self.cache_dir.mkdir(exist_ok=True)
        self.progress_file = self.cache_dir / "ultra_progress_v3.json"
        self.results_file = self.cache_dir / "ultra_results_v3.json"
        
        # ãƒãƒƒãƒå‡¦ç†è¨­å®š
        self.batch_size = 50  # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å¢—åŠ 
        self.save_interval = 100  # ä¿å­˜é–“éš”
        self.memory_limit_mb = 500  # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ä¸Šé™
        
        # åœ°åŸŸã¨ã‚¸ãƒ£ãƒ³ãƒ«ã®çµ„ã¿åˆã‚ã›
        self.genres = [
            'japanese', 'italian', 'french', 'chinese', 'korean',
            'thai', 'indian', 'spanish', 'mexican', 'vietnamese',
            'american', 'british', 'german', 'russian', 'turkish',
            'greek', 'brazilian', 'cafe', 'sweets', 'bar',
            'izakaya', 'ramen', 'sushi', 'yakiniku', 'kaiseki',
            'tempura', 'tonkatsu', 'udon', 'soba', 'curry',
            'hamburger', 'steak', 'seafood', 'vegetarian', 'vegan',
            'buffet', 'dining-bar', 'wine-bar', 'beer', 'sake'
        ]
        
        # æ±äº¬ã®è©³ç´°ã‚¨ãƒªã‚¢
        self.tokyo_areas = {
            'shinjuku': 'A1304',
            'shibuya': 'A1303', 
            'minato': 'A1301',
            'chiyoda': 'A1302',
            'chuo': 'A1313',
            'toshima': 'A1305',
            'taito': 'A1311',
            'sumida': 'A1312',
            'koto': 'A1313',
            'shinagawa': 'A1314',
            'meguro': 'A1317',
            'ota': 'A1315',
            'setagaya': 'A1318',
            'nakano': 'A1319',
            'suginami': 'A1320',
            'kita': 'A1323',
            'arakawa': 'A1324',
            'itabashi': 'A1322',
            'nerima': 'A1321',
            'adachi': 'A1324',
            'katsushika': 'A1323',
            'edogawa': 'A1313'
        }
        
    async def __aenter__(self):
        """éåŒæœŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£å…¥å£"""
        timeout = ClientTimeout(total=30, connect=10, sock_read=10)
        connector = TCPConnector(limit=self.max_concurrent, force_close=True)
        self.session = aiohttp.ClientSession(
            timeout=timeout,
            connector=connector,
            headers={
                'User-Agent': self._get_random_user_agent(),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1'
            }
        )
        
        # å‰å›ã®é€²æ—ã‚’èª­ã¿è¾¼ã‚€
        self.load_progress()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """éåŒæœŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£å‡ºå£"""
        if self.session:
            await self.session.close()
    
    def _get_random_user_agent(self) -> str:
        """ãƒ©ãƒ³ãƒ€ãƒ ãªUser-Agentã‚’å–å¾—"""
        user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        ]
        return random.choice(user_agents)
    
    def load_progress(self):
        """å‰å›ã®é€²æ—ã‚’èª­ã¿è¾¼ã‚€"""
        try:
            if self.progress_file.exists():
                with open(self.progress_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.processed_urls = set(data.get('processed_urls', []))
                    logger.info(f"å‰å›ã®é€²æ—ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {len(self.processed_urls)}ä»¶å‡¦ç†æ¸ˆã¿")
            
            if self.results_file.exists():
                with open(self.results_file, 'r', encoding='utf-8') as f:
                    self.results = json.load(f)
                    logger.info(f"å‰å›ã®çµæœã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {len(self.results)}ä»¶")
        except Exception as e:
            logger.error(f"é€²æ—èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    def save_progress(self):
        """é€²æ—ã‚’ä¿å­˜ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ï¼‰"""
        try:
            # é€²æ—ãƒ‡ãƒ¼ã‚¿ã®ã¿ä¿å­˜ï¼ˆURLãƒªã‚¹ãƒˆã‚’åœ§ç¸®ï¼‰
            with open(self.progress_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'processed_urls': list(self.processed_urls)[-10000:],  # æœ€æ–°10000ä»¶ã®ã¿ä¿æŒ
                    'total_processed': len(self.processed_urls),
                    'timestamp': datetime.now().isoformat()
                }, f, ensure_ascii=False, indent=2)
            
            # çµæœã¯è¿½è¨˜ãƒ¢ãƒ¼ãƒ‰ã§ä¿å­˜ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ï¼‰
            if len(self.results) > 1000:
                # 1000ä»¶ã”ã¨ã«åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
                chunk_num = len(self.results) // 1000
                chunk_file = self.cache_dir / f"ultra_results_v3_chunk_{chunk_num}.json"
                with open(chunk_file, 'w', encoding='utf-8') as f:
                    json.dump(self.results[-1000:], f, ensure_ascii=False, indent=2)
                # ãƒ¡ãƒ¢ãƒªã‹ã‚‰å¤ã„ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤
                self.results = self.results[-100:]
                gc.collect()  # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
            else:
                with open(self.results_file, 'w', encoding='utf-8') as f:
                    json.dump(self.results, f, ensure_ascii=False, indent=2)
                    
        except Exception as e:
            logger.error(f"é€²æ—ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    async def fetch_page(self, url: str) -> Optional[str]:
        """ãƒšãƒ¼ã‚¸ã‚’éåŒæœŸã§å–å¾—ï¼ˆã‚¨ãƒ©ãƒ¼è€æ€§å¼·åŒ–ï¼‰"""
        async with self.semaphore:
            for attempt in range(3):  # 3å›ã¾ã§ãƒªãƒˆãƒ©ã‚¤
                try:
                    # ãƒ©ãƒ³ãƒ€ãƒ ãªé…å»¶
                    delay = random.uniform(*self.delay_range)
                    await asyncio.sleep(delay)
                    
                    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’æ›´æ–°
                    self.session.headers['User-Agent'] = self._get_random_user_agent()
                    
                    async with self.session.get(url) as response:
                        if response.status == 200:
                            self.consecutive_errors = 0
                            return await response.text()
                        elif response.status == 404:
                            return None
                        elif response.status == 429:  # Rate limit
                            logger.warning(f"ãƒ¬ãƒ¼ãƒˆåˆ¶é™æ¤œå‡º: {url}")
                            await asyncio.sleep(30)  # 30ç§’å¾…æ©Ÿ
                            continue
                        else:
                            logger.warning(f"HTTPã‚¨ãƒ©ãƒ¼ {response.status}: {url}")
                            
                except asyncio.TimeoutError:
                    logger.warning(f"ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ (è©¦è¡Œ {attempt+1}/3): {url}")
                except Exception as e:
                    logger.error(f"å–å¾—ã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {attempt+1}/3) {url}: {e}")
                
                if attempt < 2:
                    await asyncio.sleep(5 * (attempt + 1))  # ãƒªãƒˆãƒ©ã‚¤å‰ã«å¾…æ©Ÿ
            
            self.errors_count += 1
            self.consecutive_errors += 1
            
            # é€£ç¶šã‚¨ãƒ©ãƒ¼ãŒå¤šã„å ´åˆã¯é•·ã‚ã«å¾…æ©Ÿ
            if self.consecutive_errors > 10:
                logger.warning(f"é€£ç¶šã‚¨ãƒ©ãƒ¼å¤šæ•°ã€‚60ç§’å¾…æ©Ÿã—ã¾ã™...")
                await asyncio.sleep(60)
                self.consecutive_errors = 0
            
            return None
    
    def build_search_urls(self) -> List[str]:
        """æ¤œç´¢URLã‚’æ§‹ç¯‰ï¼ˆã‚¸ãƒ£ãƒ³ãƒ«ã¨ã‚¨ãƒªã‚¢ã®çµ„ã¿åˆã‚ã›ï¼‰"""
        urls = []
        base_url = "https://tabelog.com"
        
        # æ±äº¬ã®å„ã‚¨ãƒªã‚¢ã¨ã‚¸ãƒ£ãƒ³ãƒ«ã®çµ„ã¿åˆã‚ã›
        for area_name, area_code in self.tokyo_areas.items():
            for genre in self.genres:
                # ã‚¸ãƒ£ãƒ³ãƒ«åˆ¥URL
                genre_url = f"{base_url}/tokyo/{area_code}/rstLst/{genre}/"
                urls.append(genre_url)
                
                # ä¾¡æ ¼å¸¯åˆ¥ã‚‚è¿½åŠ 
                for price in ['1', '2', '3', '4', '5']:  # ä¾¡æ ¼å¸¯
                    price_url = f"{base_url}/tokyo/{area_code}/rstLst/{genre}/?price={price}"
                    urls.append(price_url)
        
        # é€šå¸¸ã®ã‚¨ãƒªã‚¢åˆ¥URLã‚‚è¿½åŠ 
        for area_code in self.tokyo_areas.values():
            for page in range(1, 61):  # å„ã‚¨ãƒªã‚¢60ãƒšãƒ¼ã‚¸ã¾ã§
                area_url = f"{base_url}/tokyo/{area_code}/rstLst/{page}/"
                urls.append(area_url)
        
        # URLã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã—ã¦åã‚Šã‚’é˜²ã
        random.shuffle(urls)
        return urls
    
    async def scrape_list_page(self, url: str) -> List[str]:
        """ãƒªã‚¹ãƒˆãƒšãƒ¼ã‚¸ã‹ã‚‰åº—èˆ—URLã‚’æŠ½å‡º"""
        html = await self.fetch_page(url)
        if not html:
            return []
        
        soup = BeautifulSoup(html, 'html.parser')
        restaurant_urls = []
        
        # è¤‡æ•°ã®ã‚»ãƒ¬ã‚¯ã‚¿ã§åº—èˆ—URLã‚’æ¢ã™
        selectors = [
            'a.list-rst__rst-name-target',
            'h3.list-rst__rst-name a',
            'div.list-rst__wrap a[href*="/A"]',
            'a[class*="rst-name"]'
        ]
        
        for selector in selectors:
            links = soup.select(selector)
            for link in links:
                href = link.get('href', '')
                if href and '/A' in href and href not in self.processed_urls:
                    full_url = href if href.startswith('http') else f"https://tabelog.com{href}"
                    restaurant_urls.append(full_url)
        
        # é‡è¤‡ã‚’é™¤å»
        return list(set(restaurant_urls))
    
    async def scrape_restaurant_detail(self, url: str) -> Optional[Dict]:
        """åº—èˆ—è©³ç´°ã‚’å–å¾—"""
        if url in self.processed_urls:
            return None
        
        html = await self.fetch_page(url)
        if not html:
            return None
        
        soup = BeautifulSoup(html, 'html.parser')
        
        # åº—åå–å¾—
        shop_name = None
        name_selectors = [
            'h2.display-name span',
            'h1.rstinfo-name',
            'h1[class*="name"]',
            'div.rstinfo-table__name'
        ]
        for selector in name_selectors:
            elem = soup.select_one(selector)
            if elem:
                shop_name = elem.get_text(strip=True)
                break
        
        if not shop_name:
            return None
        
        # åŸºæœ¬æƒ…å ±ã‚’æŠ½å‡º
        info = {
            'shop_name': shop_name,
            'url': url,
            'phone': self._extract_info(soup, ['é›»è©±ç•ªå·', 'TEL', 'Tel']),
            'address': self._extract_info(soup, ['ä½æ‰€', 'æ‰€åœ¨åœ°']),
            'genre': self._extract_info(soup, ['ã‚¸ãƒ£ãƒ³ãƒ«', 'æ–™ç†']),
            'station': self._extract_info(soup, ['æœ€å¯„ã‚Šé§…', 'æœ€å¯„é§…', 'ã‚¢ã‚¯ã‚»ã‚¹']),
            'open_time': self._extract_info(soup, ['å–¶æ¥­æ™‚é–“', 'å–¶æ¥­', 'å®šä¼‘æ—¥']),
            'source': 'é£Ÿã¹ãƒ­ã‚°',
            'scraped_at': datetime.now().isoformat()
        }
        
        self.processed_urls.add(url)
        return info
    
    def _extract_info(self, soup: BeautifulSoup, keywords: List[str]) -> str:
        """æƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹æ±ç”¨ãƒ¡ã‚½ãƒƒãƒ‰"""
        for keyword in keywords:
            # ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰æ¤œç´¢
            for th in soup.find_all(['th', 'td'], string=re.compile(keyword)):
                next_elem = th.find_next_sibling()
                if next_elem:
                    text = next_elem.get_text(strip=True)
                    if text:
                        return text
            
            # ãã®ä»–ã®è¦ç´ ã‹ã‚‰æ¤œç´¢
            for elem in soup.find_all(string=re.compile(keyword)):
                parent = elem.parent
                if parent:
                    text = parent.get_text(strip=True).replace(keyword, '').strip()
                    if text and len(text) > 2:
                        return text
        
        return ""
    
    async def scrape_restaurants_ultra(self, target_count: int = 50000):
        """è¶…å¤§é‡ã®åº—èˆ—æƒ…å ±ã‚’åé›†"""
        logger.info(f"ğŸš€ ã‚¦ãƒ«ãƒˆãƒ©ãƒ¢ãƒ¼ãƒ‰èµ·å‹•: ç›®æ¨™ {target_count}ä»¶")
        
        # æ¤œç´¢URLã‚’ç”Ÿæˆ
        search_urls = self.build_search_urls()
        logger.info(f"æ¤œç´¢URLæ•°: {len(search_urls)}")
        
        # æ—¢ã«ç›®æ¨™æ•°ã«é”ã—ã¦ã„ã‚‹å ´åˆ
        current_count = len(self.results)
        if current_count >= target_count:
            logger.info(f"æ—¢ã«ç›®æ¨™æ•° {target_count}ä»¶ã‚’é”æˆã—ã¦ã„ã¾ã™ï¼ˆç¾åœ¨: {current_count}ä»¶ï¼‰")
            return self.results
        
        # ãƒãƒƒãƒå‡¦ç†ã§URLã‚’åé›†
        all_restaurant_urls = []
        for i in range(0, len(search_urls), 50):
            batch_urls = search_urls[i:i+50]
            tasks = [self.scrape_list_page(url) for url in batch_urls]
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for result in batch_results:
                if isinstance(result, list):
                    all_restaurant_urls.extend(result)
            
            # é‡è¤‡ã‚’é™¤å»
            all_restaurant_urls = list(set(all_restaurant_urls) - self.processed_urls)
            
            logger.info(f"åé›†æ¸ˆã¿URLæ•°: {len(all_restaurant_urls)}")
            
            # ååˆ†ãªURLãŒé›†ã¾ã£ãŸã‚‰è©³ç´°å–å¾—ã«ç§»ã‚‹
            if len(all_restaurant_urls) >= target_count - current_count:
                break
            
            # ãƒ¡ãƒ¢ãƒªãƒã‚§ãƒƒã‚¯
            if i % 200 == 0:
                gc.collect()
        
        # è©³ç´°æƒ…å ±ã‚’å–å¾—
        logger.info(f"è©³ç´°æƒ…å ±å–å¾—é–‹å§‹: {len(all_restaurant_urls)}ä»¶")
        
        for i in range(0, len(all_restaurant_urls), self.batch_size):
            if len(self.results) >= target_count:
                break
            
            batch_urls = all_restaurant_urls[i:i+self.batch_size]
            tasks = [self.scrape_restaurant_detail(url) for url in batch_urls]
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for result in batch_results:
                if isinstance(result, dict) and result:
                    self.results.append(result)
                    current_count = len(self.results)
                    
                    if current_count % 10 == 0:
                        logger.info(f"âœ… åé›†æ¸ˆã¿: {current_count}ä»¶")
                    
                    if current_count % self.save_interval == 0:
                        self.save_progress()
                        logger.info(f"ğŸ’¾ é€²æ—ä¿å­˜: {current_count}ä»¶")
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
            if i % 200 == 0 and i > 0:
                logger.info("â¸ï¸ ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–: 10ç§’å¾…æ©Ÿ")
                await asyncio.sleep(10)
        
        # æœ€çµ‚ä¿å­˜
        self.save_progress()
        logger.info(f"ğŸ‰ åé›†å®Œäº†: åˆè¨ˆ {len(self.results)}ä»¶")
        
        return self.results


async def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    target_count = int(sys.argv[1]) if len(sys.argv) > 1 else 50000
    
    async with TabelogUltraScraperV3(max_concurrent=100) as scraper:
        results = await scraper.scrape_restaurants_ultra(target_count)
        
        # å…¨çµæœã‚’çµ±åˆã—ã¦Excelã«å‡ºåŠ›
        from restaurant_data_integrator import RestaurantDataIntegrator
        integrator = RestaurantDataIntegrator()
        
        # ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚èª­ã¿è¾¼ã‚€
        all_results = []
        cache_dir = Path("cache")
        
        # ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        for chunk_file in sorted(cache_dir.glob("ultra_results_v3_chunk_*.json")):
            with open(chunk_file, 'r', encoding='utf-8') as f:
                chunk_data = json.load(f)
                all_results.extend(chunk_data)
        
        # æœ€æ–°ã®çµæœã‚‚è¿½åŠ 
        all_results.extend(results)
        
        # é‡è¤‡é™¤å»
        unique_results = []
        seen_urls = set()
        for r in all_results:
            if r['url'] not in seen_urls:
                unique_results.append(r)
                seen_urls.add(r['url'])
        
        logger.info(f"çµ±åˆçµæœ: {len(unique_results)}ä»¶ï¼ˆé‡è¤‡é™¤å»å¾Œï¼‰")
        
        # Excelå‡ºåŠ›
        integrator.add_restaurants(unique_results)
        integrator.remove_duplicates()
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_file = f'output/æ±äº¬éƒ½_é£²é£Ÿåº—ãƒªã‚¹ãƒˆ_ULTRA_{len(unique_results)}ä»¶_{timestamp}.xlsx'
        integrator.create_excel_report(output_file)
        
        logger.info(f"âœ… Excelãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå®Œäº†: {output_file}")
        
        # çµ±è¨ˆæƒ…å ±
        with_phone = sum(1 for r in unique_results if r.get('phone'))
        with_address = sum(1 for r in unique_results if r.get('address'))
        logger.info(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿å“è³ª:")
        logger.info(f"  - é›»è©±ç•ªå·ã‚ã‚Š: {with_phone}ä»¶ ({with_phone/len(unique_results)*100:.1f}%)")
        logger.info(f"  - ä½æ‰€ã‚ã‚Š: {with_address}ä»¶ ({with_address/len(unique_results)*100:.1f}%)")


if __name__ == "__main__":
    asyncio.run(main())