#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ—¢å­˜ã®1200ä»¶ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ æƒ…å ±ã‚’ä»˜ä¸
- å¸­æ•°ã€å…¬å¼URLã€å£ã‚³ãƒŸæ•°ã‚’è¿½åŠ å–å¾—
"""

import asyncio
import aiohttp
import json
import re
import random
import pandas as pd
from datetime import datetime
from typing import Dict, Optional
from bs4 import BeautifulSoup
from pathlib import Path
import logging
from aiohttp import ClientTimeout, TCPConnector

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('enhance_existing.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class DataEnhancer:
    """æ—¢å­˜ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.session = None
        self.semaphore = asyncio.Semaphore(3)  # åŒæ™‚æ¥ç¶šæ•°ã‚’åˆ¶é™
        self.delay = 5.0  # åŸºæœ¬é…å»¶
        
    async def __aenter__(self):
        timeout = ClientTimeout(total=30)
        self.session = aiohttp.ClientSession(
            timeout=timeout,
            headers={
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3'
            }
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def fetch_page(self, url: str) -> Optional[str]:
        """ãƒšãƒ¼ã‚¸ã‚’å–å¾—"""
        async with self.semaphore:
            await asyncio.sleep(self.delay + random.uniform(0, 2))
            
            try:
                async with self.session.get(url) as response:
                    if response.status == 200:
                        return await response.text()
                    elif response.status == 429:
                        logger.warning(f"ãƒ¬ãƒ¼ãƒˆåˆ¶é™: {url}")
                        await asyncio.sleep(30)
                        return None
                    else:
                        return None
            except Exception as e:
                logger.error(f"å–å¾—ã‚¨ãƒ©ãƒ¼ {url}: {e}")
                return None
    
    def extract_additional_info(self, soup: BeautifulSoup) -> Dict[str, str]:
        """è¿½åŠ æƒ…å ±ã‚’æŠ½å‡º"""
        info = {
            'seats': '',
            'official_url': '',
            'review_count': '0',
            'rating': '',
            'budget_dinner': '',
            'budget_lunch': ''
        }
        
        # å¸­æ•°
        for th in soup.find_all(['th', 'td'], string=re.compile('å¸­æ•°|åº§å¸­')):
            next_elem = th.find_next_sibling()
            if next_elem:
                text = next_elem.get_text(strip=True)
                if text:
                    # æ•°å­—ã‚’æŠ½å‡º
                    match = re.search(r'(\d+)', text)
                    if match:
                        info['seats'] = match.group(1) + "å¸­"
                    elif 'å¸­' in text:
                        info['seats'] = text
                    break
        
        # å…¬å¼URLï¼ˆãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸ï¼‰
        for th in soup.find_all(['th', 'td'], string=re.compile('ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸|å…¬å¼|HP')):
            next_elem = th.find_next_sibling()
            if next_elem:
                link = next_elem.find('a')
                if link and link.get('href'):
                    info['official_url'] = link.get('href')
                    break
        
        # SNSãƒªãƒ³ã‚¯ã‚‚æ¢ã™
        if not info['official_url']:
            for link in soup.find_all('a', href=True):
                href = link.get('href', '')
                if any(sns in href for sns in ['instagram.com', 'twitter.com', 'x.com', 'facebook.com']):
                    info['official_url'] = href
                    break
        
        # å£ã‚³ãƒŸæ•°
        review_elem = soup.select_one('em.rstdtl-rating__review-count, em.rdheader-rating__review-count')
        if review_elem:
            text = review_elem.get_text(strip=True)
            match = re.search(r'(\d+)', text)
            if match:
                info['review_count'] = match.group(1)
        
        # è©•ä¾¡ç‚¹
        rating_elem = soup.select_one('span.rdheader-rating__score-val-dtl, b.c-rating__val')
        if rating_elem:
            text = rating_elem.get_text(strip=True)
            match = re.search(r'(\d+\.\d+)', text)
            if match:
                info['rating'] = match.group(1)
        
        # äºˆç®—ï¼ˆå¤œï¼‰
        for elem in soup.find_all(string=re.compile('å¤œ|ãƒ‡ã‚£ãƒŠãƒ¼')):
            parent = elem.parent
            if parent:
                text = parent.get_text(strip=True)
                match = re.search(r'Â¥[\d,]+\s*[~ï½-]\s*Â¥[\d,]+', text)
                if match:
                    info['budget_dinner'] = match.group()
                    break
        
        # äºˆç®—ï¼ˆæ˜¼ï¼‰
        for elem in soup.find_all(string=re.compile('æ˜¼|ãƒ©ãƒ³ãƒ')):
            parent = elem.parent
            if parent:
                text = parent.get_text(strip=True)
                match = re.search(r'Â¥[\d,]+\s*[~ï½-]\s*Â¥[\d,]+', text)
                if match:
                    info['budget_lunch'] = match.group()
                    break
        
        return info
    
    async def enhance_restaurant(self, restaurant: Dict) -> Dict:
        """ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³æƒ…å ±ã‚’æ‹¡å¼µ"""
        url = restaurant.get('url', '')
        if not url:
            return restaurant
        
        logger.info(f"å‡¦ç†ä¸­: {restaurant.get('shop_name', 'Unknown')}")
        
        html = await self.fetch_page(url)
        if not html:
            logger.warning(f"  å–å¾—å¤±æ•—: {url}")
            return restaurant
        
        soup = BeautifulSoup(html, 'html.parser')
        additional_info = self.extract_additional_info(soup)
        
        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ æƒ…å ±ã‚’ãƒãƒ¼ã‚¸
        enhanced = restaurant.copy()
        enhanced.update(additional_info)
        
        logger.info(f"  âœ… å¸­æ•°: {additional_info['seats'] or 'N/A'}")
        logger.info(f"  âœ… å…¬å¼URL: {additional_info['official_url'] or 'N/A'}")
        logger.info(f"  âœ… å£ã‚³ãƒŸæ•°: {additional_info['review_count']}")
        logger.info(f"  âœ… è©•ä¾¡: {additional_info['rating'] or 'N/A'}")
        
        return enhanced


async def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    # æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    existing_file = Path('cache/partial_results_v2.json')
    if not existing_file.exists():
        logger.error("æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    with open(existing_file, 'r', encoding='utf-8') as f:
        existing_data = json.load(f)
    
    logger.info(f"ğŸ“Š æ—¢å­˜ãƒ‡ãƒ¼ã‚¿: {len(existing_data)}ä»¶")
    
    # ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ
    enhanced_data = []
    
    async with DataEnhancer() as enhancer:
        # ãƒãƒƒãƒå‡¦ç†
        batch_size = 10
        for i in range(0, len(existing_data), batch_size):
            batch = existing_data[i:i+batch_size]
            
            logger.info(f"\nğŸ“¦ ãƒãƒƒãƒ {i//batch_size + 1}/{(len(existing_data) + batch_size - 1)//batch_size}")
            
            tasks = [enhancer.enhance_restaurant(r) for r in batch]
            results = await asyncio.gather(*tasks)
            enhanced_data.extend(results)
            
            # é€²æ—ä¿å­˜
            if len(enhanced_data) % 50 == 0:
                with open('cache/enhanced_partial.json', 'w', encoding='utf-8') as f:
                    json.dump(enhanced_data, f, ensure_ascii=False, indent=2)
                logger.info(f"ğŸ’¾ é€²æ—ä¿å­˜: {len(enhanced_data)}ä»¶")
    
    # æœ€çµ‚ä¿å­˜
    output_file = 'cache/enhanced_results_final.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(enhanced_data, f, ensure_ascii=False, indent=2)
    
    logger.info(f"\nâœ… æ‹¡å¼µå®Œäº†: {len(enhanced_data)}ä»¶")
    
    # Excelå‡ºåŠ›
    df = pd.DataFrame(enhanced_data)
    
    # ã‚«ãƒ©ãƒ ã®é †åºã‚’æ•´ç†
    columns_order = [
        'shop_name', 'phone', 'address', 'genre', 'station', 
        'open_time', 'seats', 'official_url', 'review_count',
        'rating', 'budget_dinner', 'budget_lunch', 'url', 'source', 'scraped_at'
    ]
    
    # å­˜åœ¨ã™ã‚‹ã‚«ãƒ©ãƒ ã®ã¿é¸æŠ
    existing_columns = [col for col in columns_order if col in df.columns]
    df = df[existing_columns]
    
    # Excelå‡ºåŠ›
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    excel_file = f'output/æ±äº¬éƒ½_é£²é£Ÿåº—ãƒªã‚¹ãƒˆ_æ‹¡å¼µ_1200ä»¶_{timestamp}.xlsx'
    
    with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:
        df.to_excel(writer, sheet_name='é£²é£Ÿåº—ãƒªã‚¹ãƒˆ', index=False)
        
        # ã‚«ãƒ©ãƒ å¹…èª¿æ•´
        worksheet = writer.sheets['é£²é£Ÿåº—ãƒªã‚¹ãƒˆ']
        for column in worksheet.columns:
            max_length = 0
            column_letter = column[0].column_letter
            for cell in column:
                try:
                    if len(str(cell.value)) > max_length:
                        max_length = len(str(cell.value))
                except:
                    pass
            adjusted_width = min(max_length + 2, 50)
            worksheet.column_dimensions[column_letter].width = adjusted_width
    
    logger.info(f"ğŸ“Š Excelãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ: {excel_file}")
    
    # ãƒ‡ãƒ¼ã‚¿å“è³ªçµ±è¨ˆ
    with_seats = sum(1 for r in enhanced_data if r.get('seats'))
    with_official = sum(1 for r in enhanced_data if r.get('official_url'))
    with_reviews = sum(1 for r in enhanced_data if r.get('review_count') and r['review_count'] != '0')
    with_rating = sum(1 for r in enhanced_data if r.get('rating'))
    
    logger.info(f"\nğŸ“ˆ ãƒ‡ãƒ¼ã‚¿å“è³ªçµ±è¨ˆ:")
    logger.info(f"  å¸­æ•°æƒ…å ±: {with_seats}/{len(enhanced_data)} ({with_seats/len(enhanced_data)*100:.1f}%)")
    logger.info(f"  å…¬å¼URL: {with_official}/{len(enhanced_data)} ({with_official/len(enhanced_data)*100:.1f}%)")
    logger.info(f"  å£ã‚³ãƒŸã‚ã‚Š: {with_reviews}/{len(enhanced_data)} ({with_reviews/len(enhanced_data)*100:.1f}%)")
    logger.info(f"  è©•ä¾¡ç‚¹: {with_rating}/{len(enhanced_data)} ({with_rating/len(enhanced_data)*100:.1f}%)")


if __name__ == "__main__":
    asyncio.run(main())