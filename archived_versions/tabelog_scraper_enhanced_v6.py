#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
é£Ÿã¹ãƒ­ã‚°ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ æ‹¡å¼µç‰ˆ V6
- å¸­æ•°ã€å…¬å¼URLã€å£ã‚³ãƒŸæ•°ã‚’å«ã‚€è©³ç´°æƒ…å ±ã‚’å–å¾—
- æ—¢å­˜ã®1200ä»¶ãƒ‡ãƒ¼ã‚¿ã‚’æ‹¡å¼µ
"""

import asyncio
import aiohttp
import json
import re
import random
import time
from datetime import datetime
from typing import Dict, List, Optional, Set
from bs4 import BeautifulSoup
from pathlib import Path
import logging
from aiohttp import ClientTimeout, TCPConnector

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('tabelog_enhanced_v6.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class TabelogEnhancedScraperV6:
    """æ‹¡å¼µç‰ˆé£Ÿã¹ãƒ­ã‚°ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
    
    def __init__(self, max_concurrent: int = 5):
        """åˆæœŸåŒ–"""
        self.max_concurrent = max_concurrent
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.session = None
        self.delay_range = (3.0, 5.0)  # é©åº¦ãªé…å»¶
        
        # é€²æ—ç®¡ç†
        self.processed_urls: Set[str] = set()
        self.results: List[Dict] = []
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        self.cache_dir = Path("cache")
        self.cache_dir.mkdir(exist_ok=True)
        self.progress_file = self.cache_dir / "enhanced_progress_v6.json"
        self.results_file = self.cache_dir / "enhanced_results_v6.json"
        
    async def __aenter__(self):
        """éåŒæœŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£å…¥å£"""
        timeout = ClientTimeout(total=30, connect=10, sock_read=10)
        connector = TCPConnector(limit=self.max_concurrent, force_close=True)
        self.session = aiohttp.ClientSession(
            timeout=timeout,
            connector=connector,
            headers=self._get_headers()
        )
        
        # é€²æ—ã‚’èª­ã¿è¾¼ã‚€
        self.load_progress()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """éåŒæœŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£å‡ºå£"""
        if self.session:
            await self.session.close()
    
    def _get_headers(self) -> Dict[str, str]:
        """ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ˜ãƒƒãƒ€ãƒ¼ã‚’å–å¾—"""
        user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/120.0'
        ]
        
        return {
            'User-Agent': random.choice(user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
    
    def load_progress(self):
        """å‰å›ã®é€²æ—ã‚’èª­ã¿è¾¼ã‚€"""
        try:
            if self.progress_file.exists():
                with open(self.progress_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.processed_urls = set(data.get('processed_urls', []))
                    logger.info(f"å‰å›ã®é€²æ—ã‚’èª­ã¿è¾¼ã¿: {len(self.processed_urls)}ä»¶å‡¦ç†æ¸ˆã¿")
            
            if self.results_file.exists():
                with open(self.results_file, 'r', encoding='utf-8') as f:
                    self.results = json.load(f)
                    logger.info(f"å‰å›ã®çµæœã‚’èª­ã¿è¾¼ã¿: {len(self.results)}ä»¶")
                
        except Exception as e:
            logger.error(f"é€²æ—èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    def save_progress(self):
        """é€²æ—ã‚’ä¿å­˜"""
        try:
            with open(self.progress_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'processed_urls': list(self.processed_urls),
                    'total_processed': len(self.processed_urls),
                    'timestamp': datetime.now().isoformat()
                }, f, ensure_ascii=False, indent=2)
            
            with open(self.results_file, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, ensure_ascii=False, indent=2)
                    
        except Exception as e:
            logger.error(f"é€²æ—ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    async def fetch_page(self, url: str) -> Optional[str]:
        """ãƒšãƒ¼ã‚¸ã‚’éåŒæœŸã§å–å¾—"""
        async with self.semaphore:
            try:
                # ãƒ©ãƒ³ãƒ€ãƒ ãªé…å»¶
                delay = random.uniform(*self.delay_range)
                await asyncio.sleep(delay)
                
                # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’æ›´æ–°
                self.session.headers['User-Agent'] = random.choice([
                    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
                    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36'
                ])
                
                async with self.session.get(url) as response:
                    if response.status == 200:
                        return await response.text()
                    elif response.status == 429:
                        logger.warning(f"ãƒ¬ãƒ¼ãƒˆåˆ¶é™æ¤œå‡º: {url}")
                        await asyncio.sleep(30)
                        return None
                    else:
                        logger.warning(f"HTTPã‚¨ãƒ©ãƒ¼ {response.status}: {url}")
                        return None
                        
            except asyncio.TimeoutError:
                logger.error(f"ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {url}")
                return None
            except Exception as e:
                logger.error(f"å–å¾—ã‚¨ãƒ©ãƒ¼ {url}: {e}")
                return None
    
    async def scrape_restaurant_enhanced(self, restaurant_url: str) -> Optional[Dict]:
        """åº—èˆ—ã®æ‹¡å¼µæƒ…å ±ã‚’å–å¾—"""
        
        if restaurant_url in self.processed_urls:
            return None
        
        html = await self.fetch_page(restaurant_url)
        if not html:
            return None
        
        soup = BeautifulSoup(html, 'html.parser')
        
        # åŸºæœ¬æƒ…å ±å–å¾—
        shop_name = self._extract_shop_name(soup)
        if not shop_name:
            return None
        
        # æ‹¡å¼µæƒ…å ±ã‚’å«ã‚€è©³ç´°ãƒ‡ãƒ¼ã‚¿
        info = {
            'shop_name': shop_name,
            'url': restaurant_url,
            'phone': self._extract_phone(soup),
            'address': self._extract_address(soup),
            'genre': self._extract_genre(soup),
            'station': self._extract_station(soup),
            'open_time': self._extract_open_time(soup),
            # æ–°è¦è¿½åŠ é …ç›®
            'seats': self._extract_seats(soup),
            'official_url': self._extract_official_url(soup),
            'review_count': self._extract_review_count(soup),
            'rating': self._extract_rating(soup),
            'budget_dinner': self._extract_budget(soup, 'dinner'),
            'budget_lunch': self._extract_budget(soup, 'lunch'),
            'source': 'é£Ÿã¹ãƒ­ã‚°',
            'scraped_at': datetime.now().isoformat()
        }
        
        self.processed_urls.add(restaurant_url)
        return info
    
    def _extract_shop_name(self, soup: BeautifulSoup) -> str:
        """åº—åã‚’æŠ½å‡º"""
        selectors = [
            'h2.display-name span',
            'h2.display-name',
            'h1.display-name span',
            'h1.display-name',
            '.rstinfo-table__name'
        ]
        
        for selector in selectors:
            elem = soup.select_one(selector)
            if elem:
                text = elem.get_text(strip=True)
                text = re.sub(r'\s*\([^)]+\)\s*$', '', text)
                text = re.sub(r'\s+', ' ', text)
                if text and text != 'é£Ÿã¹ãƒ­ã‚°':
                    return text.strip()
        
        return ""
    
    def _extract_phone(self, soup: BeautifulSoup) -> str:
        """é›»è©±ç•ªå·ã‚’æŠ½å‡º"""
        phone_patterns = [
            r'03-\d{4}-\d{4}',
            r'0\d{2,3}-\d{3,4}-\d{4}',
            r'0\d{9,10}'
        ]
        
        selectors = [
            'span.rstinfo-table__tel-num',
            'p.rstinfo-table__tel'
        ]
        
        for selector in selectors:
            elem = soup.select_one(selector)
            if elem:
                text = elem.get_text(strip=True)
                for pattern in phone_patterns:
                    match = re.search(pattern, text)
                    if match:
                        return match.group()
        
        return ""
    
    def _extract_address(self, soup: BeautifulSoup) -> str:
        """ä½æ‰€ã‚’æŠ½å‡º"""
        selectors = [
            'p.rstinfo-table__address',
            'span.rstinfo-table__address'
        ]
        
        for selector in selectors:
            elem = soup.select_one(selector)
            if elem:
                address = elem.get_text(strip=True)
                address = re.sub(r'ã€’\d{3}-\d{4}\s*', '', address)
                if address:
                    return address
        
        return ""
    
    def _extract_genre(self, soup: BeautifulSoup) -> str:
        """ã‚¸ãƒ£ãƒ³ãƒ«ã‚’æŠ½å‡º"""
        # ã‚¸ãƒ£ãƒ³ãƒ«å°‚ç”¨ã‚»ãƒ¬ã‚¯ã‚¿
        for th in soup.find_all(['th', 'td'], string=re.compile('ã‚¸ãƒ£ãƒ³ãƒ«')):
            next_elem = th.find_next_sibling()
            if next_elem:
                genre = next_elem.get_text(strip=True)
                if genre:
                    return genre
        
        # ãã®ä»–ã®ã‚»ãƒ¬ã‚¯ã‚¿
        genre_elem = soup.select_one('.rstinfo-table__genre')
        if genre_elem:
            return genre_elem.get_text(strip=True)
        
        return ""
    
    def _extract_station(self, soup: BeautifulSoup) -> str:
        """æœ€å¯„ã‚Šé§…ã‚’æŠ½å‡º"""
        for th in soup.find_all(['th', 'td'], string=re.compile('äº¤é€šæ‰‹æ®µ|æœ€å¯„ã‚Šé§…')):
            next_elem = th.find_next_sibling()
            if next_elem:
                station = next_elem.get_text(strip=True)
                station = station.split('ã€')[0].split('ã‹ã‚‰')[0]
                if station:
                    return station
        
        return ""
    
    def _extract_open_time(self, soup: BeautifulSoup) -> str:
        """å–¶æ¥­æ™‚é–“ã‚’æŠ½å‡º"""
        for th in soup.find_all(['th', 'td'], string=re.compile('å–¶æ¥­æ™‚é–“')):
            next_elem = th.find_next_sibling()
            if next_elem:
                hours = next_elem.get_text(strip=True)
                hours = re.sub(r'\s+', ' ', hours)
                if hours:
                    return hours
        
        hours_elem = soup.select_one('p.rstinfo-table__open-hours')
        if hours_elem:
            return re.sub(r'\s+', ' ', hours_elem.get_text(strip=True))
        
        return ""
    
    def _extract_seats(self, soup: BeautifulSoup) -> str:
        """å¸­æ•°ã‚’æŠ½å‡º"""
        # å¸­æ•°ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
        seat_patterns = [
            r'(\d+)\s*å¸­',
            r'å¸­æ•°\s*[:ï¼š]\s*(\d+)',
            r'(\d+)\s*seats'
        ]
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰æ¤œç´¢
        for th in soup.find_all(['th', 'td'], string=re.compile('å¸­æ•°|åº§å¸­')):
            next_elem = th.find_next_sibling()
            if next_elem:
                text = next_elem.get_text(strip=True)
                for pattern in seat_patterns:
                    match = re.search(pattern, text)
                    if match:
                        return match.group(1) + "å¸­"
                # ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ãƒãƒƒãƒã—ãªãã¦ã‚‚å¸­æ•°æƒ…å ±ãŒã‚ã‚Œã°è¿”ã™
                if text and 'å¸­' in text:
                    return text
        
        # rstinfo-tableã‹ã‚‰æ¤œç´¢
        info_table = soup.select_one('.rstinfo-table')
        if info_table:
            text = info_table.get_text()
            for pattern in seat_patterns:
                match = re.search(pattern, text)
                if match:
                    return match.group(1) + "å¸­"
        
        return ""
    
    def _extract_official_url(self, soup: BeautifulSoup) -> str:
        """å…¬å¼ã‚µã‚¤ãƒˆURLã‚’æŠ½å‡º"""
        # å…¬å¼ã‚µã‚¤ãƒˆãƒªãƒ³ã‚¯ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
        official_patterns = [
            'ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸',
            'å…¬å¼',
            'Official',
            'HP',
            'Website'
        ]
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰æ¤œç´¢
        for pattern in official_patterns:
            for th in soup.find_all(['th', 'td'], string=re.compile(pattern, re.IGNORECASE)):
                next_elem = th.find_next_sibling()
                if next_elem:
                    link = next_elem.find('a')
                    if link and link.get('href'):
                        return link.get('href')
        
        # å¤–éƒ¨ãƒªãƒ³ã‚¯ã‹ã‚‰æ¤œç´¢
        for link in soup.find_all('a', href=True):
            href = link.get('href', '')
            text = link.get_text(strip=True)
            # é£Ÿã¹ãƒ­ã‚°ä»¥å¤–ã®å¤–éƒ¨ãƒªãƒ³ã‚¯ã§å…¬å¼ã£ã½ã„ã‚‚ã®
            if 'tabelog.com' not in href and any(p in text for p in official_patterns):
                if href.startswith('http'):
                    return href
        
        # SNSãƒªãƒ³ã‚¯ï¼ˆInstagram, Twitter/X, Facebookï¼‰
        sns_links = []
        for link in soup.find_all('a', href=True):
            href = link.get('href', '')
            if any(sns in href for sns in ['instagram.com', 'twitter.com', 'x.com', 'facebook.com']):
                sns_links.append(href)
        
        # SNSãƒªãƒ³ã‚¯ãŒã‚ã‚Œã°æœ€åˆã®ã‚‚ã®ã‚’è¿”ã™
        if sns_links:
            return sns_links[0]
        
        return ""
    
    def _extract_review_count(self, soup: BeautifulSoup) -> str:
        """å£ã‚³ãƒŸæ•°ã‚’æŠ½å‡º"""
        # å£ã‚³ãƒŸæ•°ã®ã‚»ãƒ¬ã‚¯ã‚¿
        review_selectors = [
            'em.rstdtl-rating__review-count',
            'span.rstdtl-rating__review-count',
            '.rdheader-rating__review-count em',
            'em.rdheader-rating__review-count'
        ]
        
        for selector in review_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = elem.get_text(strip=True)
                # æ•°å­—ã®ã¿æŠ½å‡º
                match = re.search(r'(\d+)', text)
                if match:
                    return match.group(1)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
        review_patterns = [
            r'(\d+)\s*ä»¶ã®å£ã‚³ãƒŸ',
            r'å£ã‚³ãƒŸ\s*(\d+)\s*ä»¶',
            r'ãƒ¬ãƒ“ãƒ¥ãƒ¼\s*(\d+)',
            r'(\d+)\s*reviews'
        ]
        
        text = soup.get_text()
        for pattern in review_patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(1)
        
        return "0"
    
    def _extract_rating(self, soup: BeautifulSoup) -> str:
        """è©•ä¾¡ç‚¹ã‚’æŠ½å‡º"""
        rating_selectors = [
            'span.rdheader-rating__score-val-dtl',
            'b.c-rating__val',
            'span.rstdtl-rating__score',
            '.rdheader-rating__score em'
        ]
        
        for selector in rating_selectors:
            elem = soup.select_one(selector)
            if elem:
                text = elem.get_text(strip=True)
                # æ•°å€¤ã®ã¿æŠ½å‡ºï¼ˆ3.58 ã®ã‚ˆã†ãªå½¢å¼ï¼‰
                match = re.search(r'(\d+\.\d+)', text)
                if match:
                    return match.group(1)
        
        return ""
    
    def _extract_budget(self, soup: BeautifulSoup, meal_type: str) -> str:
        """äºˆç®—ã‚’æŠ½å‡º"""
        if meal_type == 'dinner':
            keywords = ['å¤œ', 'ãƒ‡ã‚£ãƒŠãƒ¼', 'å¤•é£Ÿ']
        else:
            keywords = ['æ˜¼', 'ãƒ©ãƒ³ãƒ', 'æ˜¼é£Ÿ']
        
        # äºˆç®—æƒ…å ±ã‚’æ¢ã™
        for keyword in keywords:
            for elem in soup.find_all(string=re.compile(keyword)):
                parent = elem.parent
                if parent:
                    text = parent.get_text(strip=True)
                    # ä¾¡æ ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆÂ¥1,000ï½Â¥1,999 ãªã©ï¼‰
                    match = re.search(r'Â¥[\d,]+\s*[~ï½-]\s*Â¥[\d,]+', text)
                    if match:
                        return match.group()
        
        return ""
    
    async def scrape_restaurant_list(self, area_url: str, max_pages: int = 10) -> List[str]:
        """ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³ãƒªã‚¹ãƒˆãƒšãƒ¼ã‚¸ã‹ã‚‰åº—èˆ—URLã‚’å–å¾—"""
        restaurant_urls = []
        
        for page in range(1, max_pages + 1):
            page_url = f"{area_url}rstLst/{page}/"
            html = await self.fetch_page(page_url)
            
            if not html:
                continue
            
            soup = BeautifulSoup(html, 'html.parser')
            
            # åº—èˆ—ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
            links = soup.select('a.list-rst__rst-name-target')
            if not links:
                links = soup.select('h3.list-rst__rst-name a')
            
            for link in links:
                href = link.get('href', '')
                if href:
                    full_url = href if href.startswith('http') else f"https://tabelog.com{href}"
                    restaurant_urls.append(full_url)
            
            logger.info(f"ãƒšãƒ¼ã‚¸ {page}: {len(links)}ä»¶ã®åº—èˆ—URLå–å¾—")
        
        return restaurant_urls
    
    async def scrape_enhanced_data(self, target_count: int = 1200):
        """æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ã‚’åé›†"""
        logger.info(f"ğŸš€ æ‹¡å¼µç‰ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼èµ·å‹•: ç›®æ¨™ {target_count}ä»¶")
        
        # æ±äº¬ã®ä¸»è¦ã‚¨ãƒªã‚¢
        area_urls = [
            'https://tabelog.com/tokyo/A1301/',  # éŠ€åº§ãƒ»æ–°æ©‹
            'https://tabelog.com/tokyo/A1302/',  # æ±äº¬ãƒ»ä¸¸ã®å†…
            'https://tabelog.com/tokyo/A1303/',  # æ¸‹è°·
            'https://tabelog.com/tokyo/A1304/',  # æ–°å®¿
            'https://tabelog.com/tokyo/A1305/',  # æ± è¢‹
            'https://tabelog.com/tokyo/A1306/',  # åŸå®¿ãƒ»è¡¨å‚é“
            'https://tabelog.com/tokyo/A1307/',  # å…­æœ¬æœ¨ãƒ»éº»å¸ƒ
            'https://tabelog.com/tokyo/A1308/',  # èµ¤å‚
            'https://tabelog.com/tokyo/A1309/',  # å››ãƒ„è°·ãƒ»å¸‚ãƒ¶è°·
            'https://tabelog.com/tokyo/A1310/',  # ç§‹è‘‰åŸãƒ»ç¥ç”°
        ]
        
        all_urls = []
        
        # å„ã‚¨ãƒªã‚¢ã‹ã‚‰åº—èˆ—URLã‚’åé›†
        for area_url in area_urls:
            if len(all_urls) >= target_count * 1.2:  # 20%ä½™åˆ†ã«åé›†
                break
            
            area_name = area_url.split('/')[-2]
            logger.info(f"ğŸ“ ã‚¨ãƒªã‚¢ {area_name} ã®ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹")
            
            urls = await self.scrape_restaurant_list(area_url, max_pages=10)
            all_urls.extend(urls)
            logger.info(f"  åˆè¨ˆ {len(all_urls)} ä»¶ã®URLåé›†æ¸ˆã¿")
        
        # é‡è¤‡ã‚’é™¤å»
        all_urls = list(set(all_urls))[:target_count]
        logger.info(f"ğŸ“‹ {len(all_urls)}ä»¶ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯URLã‚’å‡¦ç†ã—ã¾ã™")
        
        # è©³ç´°æƒ…å ±ã‚’å–å¾—
        for i, url in enumerate(all_urls):
            if len(self.results) >= target_count:
                break
            
            if url in self.processed_urls:
                continue
            
            logger.info(f"å‡¦ç†ä¸­ {i+1}/{len(all_urls)}: {url}")
            result = await self.scrape_restaurant_enhanced(url)
            
            if result:
                self.results.append(result)
                logger.info(f"  âœ… åé›†æˆåŠŸ: {result['shop_name']}")
                logger.info(f"    å¸­æ•°: {result['seats'] or 'N/A'}")
                logger.info(f"    å…¬å¼URL: {result['official_url'] or 'N/A'}")
                logger.info(f"    å£ã‚³ãƒŸæ•°: {result['review_count']}")
                
                # 10ä»¶ã”ã¨ã«ä¿å­˜
                if len(self.results) % 10 == 0:
                    self.save_progress()
                    logger.info(f"ğŸ’¾ é€²æ—ä¿å­˜: {len(self.results)}ä»¶")
        
        # æœ€çµ‚ä¿å­˜
        self.save_progress()
        logger.info(f"ğŸ‰ åé›†å®Œäº†: åˆè¨ˆ {len(self.results)}ä»¶")
        
        return self.results


async def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    async with TabelogEnhancedScraperV6(max_concurrent=5) as scraper:
        results = await scraper.scrape_enhanced_data(target_count=1200)
        
        if results:
            # Excelå‡ºåŠ›
            from restaurant_data_integrator import RestaurantDataIntegrator
            integrator = RestaurantDataIntegrator()
            
            # æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ç”¨ã«integratorã‚’ä¿®æ­£ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ãŸã‚ã€
            # ã“ã“ã§ã¯ç›´æ¥pandasã‚’ä½¿ç”¨
            import pandas as pd
            
            df = pd.DataFrame(results)
            
            # ã‚«ãƒ©ãƒ ã®é †åºã‚’æ•´ç†
            columns = [
                'shop_name', 'phone', 'address', 'genre', 'station', 
                'open_time', 'seats', 'official_url', 'review_count',
                'rating', 'budget_dinner', 'budget_lunch', 'url', 'source', 'scraped_at'
            ]
            
            # å­˜åœ¨ã™ã‚‹ã‚«ãƒ©ãƒ ã®ã¿é¸æŠ
            existing_columns = [col for col in columns if col in df.columns]
            df = df[existing_columns]
            
            # Excelå‡ºåŠ›
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_file = f'output/æ±äº¬éƒ½_é£²é£Ÿåº—ãƒªã‚¹ãƒˆ_æ‹¡å¼µ_{len(results)}ä»¶_{timestamp}.xlsx'
            
            with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
                df.to_excel(writer, sheet_name='é£²é£Ÿåº—ãƒªã‚¹ãƒˆ', index=False)
                
                # ã‚«ãƒ©ãƒ å¹…ã‚’è‡ªå‹•èª¿æ•´
                worksheet = writer.sheets['é£²é£Ÿåº—ãƒªã‚¹ãƒˆ']
                for column in worksheet.columns:
                    max_length = 0
                    column_letter = column[0].column_letter
                    for cell in column:
                        try:
                            if len(str(cell.value)) > max_length:
                                max_length = len(str(cell.value))
                        except:
                            pass
                    adjusted_width = min(max_length + 2, 50)
                    worksheet.column_dimensions[column_letter].width = adjusted_width
            
            logger.info(f"âœ… Excelãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ: {output_file}")
            
            # ãƒ‡ãƒ¼ã‚¿å“è³ªçµ±è¨ˆ
            with_seats = sum(1 for r in results if r.get('seats'))
            with_official = sum(1 for r in results if r.get('official_url'))
            with_reviews = sum(1 for r in results if r.get('review_count') and r['review_count'] != '0')
            with_rating = sum(1 for r in results if r.get('rating'))
            
            logger.info(f"\nğŸ“Š ãƒ‡ãƒ¼ã‚¿å“è³ª:")
            logger.info(f"  å¸­æ•°æƒ…å ±: {with_seats}/{len(results)} ({with_seats/len(results)*100:.1f}%)")
            logger.info(f"  å…¬å¼URL: {with_official}/{len(results)} ({with_official/len(results)*100:.1f}%)")
            logger.info(f"  å£ã‚³ãƒŸã‚ã‚Š: {with_reviews}/{len(results)} ({with_reviews/len(results)*100:.1f}%)")
            logger.info(f"  è©•ä¾¡ç‚¹: {with_rating}/{len(results)} ({with_rating/len(results)*100:.1f}%)")


if __name__ == "__main__":
    asyncio.run(main())