#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
é£Ÿã¹ãƒ­ã‚°ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ ã‚¹ãƒãƒ¼ãƒˆç‰ˆ V4
- ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«é©å¿œçš„ã«å¯¾å¿œ
- æ®µéšçš„ãªã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æˆ¦ç•¥
- å®Ÿç¸¾ã®ã‚ã‚‹v2ç‰ˆã‚’ãƒ™ãƒ¼ã‚¹ã«å¤§é‡åé›†å¯¾å¿œ
"""

import asyncio
import aiohttp
import json
import re
import random
import time
import os
import sys
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Set, Tuple
from bs4 import BeautifulSoup
from pathlib import Path
import logging
from aiohttp import ClientTimeout, TCPConnector

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('tabelog_smart_v4.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class TabelogSmartScraperV4:
    """ã‚¹ãƒãƒ¼ãƒˆç‰ˆé£Ÿã¹ãƒ­ã‚°ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ - ãƒ¬ãƒ¼ãƒˆåˆ¶é™é©å¿œå‹"""
    
    def __init__(self, max_concurrent: int = 20):
        """åˆæœŸåŒ–"""
        self.max_concurrent = max_concurrent
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.session = None
        
        # é©å¿œçš„ãªé…å»¶ç®¡ç†
        self.min_delay = 1.0
        self.current_delay = 2.0
        self.max_delay = 10.0
        self.rate_limit_count = 0
        self.success_count = 0
        
        # é€²æ—ç®¡ç†
        self.processed_urls: Set[str] = set()
        self.results: List[Dict] = []
        self.errors_count = 0
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        self.cache_dir = Path("cache")
        self.cache_dir.mkdir(exist_ok=True)
        self.progress_file = self.cache_dir / "smart_progress_v4.json"
        self.results_dir = Path("output") / "smart_results_v4"
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        # æ±äº¬ã®ã‚¨ãƒªã‚¢ï¼ˆã‚·ãƒ³ãƒ—ãƒ«åŒ–ï¼‰
        self.tokyo_main_areas = {
            'æ–°å®¿ãƒ»ä»£ã€…æœ¨ãƒ»å¤§ä¹…ä¿': 'https://tabelog.com/tokyo/A1304/',
            'æ¸‹è°·': 'https://tabelog.com/tokyo/A1303/',
            'æµæ¯”å¯¿ãƒ»ç›®é»’ãƒ»å“å·': 'https://tabelog.com/tokyo/A1316/',
            'éŠ€åº§ãƒ»æ–°æ©‹ãƒ»æœ‰æ¥½ç”º': 'https://tabelog.com/tokyo/A1301/',
            'æ±äº¬ãƒ»ä¸¸ã®å†…ãƒ»æ—¥æœ¬æ©‹': 'https://tabelog.com/tokyo/A1302/',
            'ä¸Šé‡ãƒ»æµ…è‰ãƒ»æ—¥æš®é‡Œ': 'https://tabelog.com/tokyo/A1311/',
            'æ± è¢‹ï½é«˜ç”°é¦¬å ´ãƒ»æ—©ç¨²ç”°': 'https://tabelog.com/tokyo/A1305/',
            'åŸå®¿ãƒ»è¡¨å‚é“ãƒ»é’å±±': 'https://tabelog.com/tokyo/A1306/',
            'å…­æœ¬æœ¨ãƒ»éº»å¸ƒãƒ»åºƒå°¾': 'https://tabelog.com/tokyo/A1307/',
            'èµ¤å‚ãƒ»æ°¸ç”°ç”ºãƒ»æºœæ± ': 'https://tabelog.com/tokyo/A1308/',
            'å››ãƒ„è°·ãƒ»å¸‚ãƒ¶è°·ãƒ»é£¯ç”°æ©‹': 'https://tabelog.com/tokyo/A1309/',
            'ç§‹è‘‰åŸãƒ»ç¥ç”°ãƒ»æ°´é“æ©‹': 'https://tabelog.com/tokyo/A1310/',
            'éŒ¦ç³¸ç”ºãƒ»æŠ¼ä¸Šãƒ»æ–°å°å²©': 'https://tabelog.com/tokyo/A1312/',
            'è‘›é£¾ãƒ»æ±Ÿæˆ¸å·ãƒ»æ±Ÿæ±': 'https://tabelog.com/tokyo/A1313/',
            'è’²ç”°ãƒ»å¤§æ£®ãƒ»ç¾½ç”°å‘¨è¾º': 'https://tabelog.com/tokyo/A1315/',
            'è‡ªç”±ãŒä¸˜ãƒ»ä¸­ç›®é»’ãƒ»å­¦èŠ¸å¤§å­¦': 'https://tabelog.com/tokyo/A1317/',
            'ä¸‹åŒ—æ²¢ãƒ»æ˜å¤§å‰ãƒ»æˆåŸå­¦åœ’å‰': 'https://tabelog.com/tokyo/A1318/',
            'ä¸­é‡ãƒ»å‰ç¥¥å¯ºãƒ»ä¸‰é·¹': 'https://tabelog.com/tokyo/A1319/',
            'è¥¿è»çªªãƒ»è»çªªãƒ»é˜¿ä½ãƒ¶è°·': 'https://tabelog.com/tokyo/A1320/',
            'æ¿æ©‹ãƒ»æ±æ­¦ç·´é¦¬ãƒ»ä¸‹èµ¤å¡š': 'https://tabelog.com/tokyo/A1322/',
            'å¤§å¡šãƒ»å·£é´¨ãƒ»é§’è¾¼ãƒ»èµ¤ç¾½': 'https://tabelog.com/tokyo/A1323/',
            'åƒä½ãƒ»ç¶¾ç€¬ãƒ»è‘›é£¾': 'https://tabelog.com/tokyo/A1324/'
        }
        
    async def __aenter__(self):
        """éåŒæœŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£å…¥å£"""
        timeout = ClientTimeout(total=30, connect=10, sock_read=10)
        connector = TCPConnector(limit=self.max_concurrent, force_close=True)
        self.session = aiohttp.ClientSession(
            timeout=timeout,
            connector=connector,
            headers=self._get_headers()
        )
        
        # å‰å›ã®é€²æ—ã‚’èª­ã¿è¾¼ã‚€
        self.load_progress()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """éåŒæœŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£å‡ºå£"""
        if self.session:
            await self.session.close()
    
    def _get_headers(self) -> Dict[str, str]:
        """ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ˜ãƒƒãƒ€ãƒ¼ã‚’å–å¾—"""
        return {
            'User-Agent': self._get_random_user_agent(),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Cache-Control': 'max-age=0'
        }
    
    def _get_random_user_agent(self) -> str:
        """ãƒ©ãƒ³ãƒ€ãƒ ãªUser-Agentã‚’å–å¾—"""
        user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/120.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        ]
        return random.choice(user_agents)
    
    def load_progress(self):
        """å‰å›ã®é€²æ—ã‚’èª­ã¿è¾¼ã‚€"""
        try:
            if self.progress_file.exists():
                with open(self.progress_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.processed_urls = set(data.get('processed_urls', []))
                    logger.info(f"å‰å›ã®é€²æ—ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {len(self.processed_urls)}ä»¶å‡¦ç†æ¸ˆã¿")
            
            # æ—¢å­˜ã®çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            existing_results = list(self.results_dir.glob("results_*.json"))
            if existing_results:
                total_count = 0
                for result_file in existing_results:
                    with open(result_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        total_count += len(data)
                logger.info(f"æ—¢å­˜ã®çµæœãƒ•ã‚¡ã‚¤ãƒ«: {len(existing_results)}å€‹ã€åˆè¨ˆ{total_count}ä»¶")
                
        except Exception as e:
            logger.error(f"é€²æ—èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    def save_progress(self):
        """é€²æ—ã‚’ä¿å­˜"""
        try:
            with open(self.progress_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'processed_urls': list(self.processed_urls)[-50000:],  # æœ€æ–°50000ä»¶ã®ã¿ä¿æŒ
                    'total_processed': len(self.processed_urls),
                    'timestamp': datetime.now().isoformat(),
                    'current_delay': self.current_delay,
                    'rate_limit_count': self.rate_limit_count
                }, f, ensure_ascii=False, indent=2)
            
            # çµæœã‚’ãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆ1000ä»¶ã”ã¨ï¼‰
            if self.results and len(self.results) >= 1000:
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                chunk_file = self.results_dir / f"results_{timestamp}_{len(self.results)}.json"
                with open(chunk_file, 'w', encoding='utf-8') as f:
                    json.dump(self.results, f, ensure_ascii=False, indent=2)
                logger.info(f"ğŸ’¾ çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {chunk_file}")
                self.results = []  # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
                    
        except Exception as e:
            logger.error(f"é€²æ—ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    async def fetch_with_adaptive_delay(self, url: str) -> Optional[str]:
        """é©å¿œçš„ãªé…å»¶ã§ãƒšãƒ¼ã‚¸ã‚’å–å¾—"""
        async with self.semaphore:
            # é©å¿œçš„ãªé…å»¶
            await asyncio.sleep(self.current_delay + random.uniform(0, 1))
            
            try:
                # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’æ›´æ–°
                self.session.headers['User-Agent'] = self._get_random_user_agent()
                
                async with self.session.get(url) as response:
                    if response.status == 200:
                        # æˆåŠŸã—ãŸã‚‰é…å»¶ã‚’æ¸›ã‚‰ã™
                        self.success_count += 1
                        if self.success_count > 10:
                            self.current_delay = max(self.min_delay, self.current_delay * 0.9)
                            self.success_count = 0
                        return await response.text()
                    
                    elif response.status == 429:
                        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™æ¤œå‡º
                        self.rate_limit_count += 1
                        self.current_delay = min(self.max_delay, self.current_delay * 1.5)
                        logger.warning(f"ãƒ¬ãƒ¼ãƒˆåˆ¶é™æ¤œå‡º (é…å»¶ã‚’{self.current_delay:.1f}ç§’ã«èª¿æ•´)")
                        
                        # é•·æ™‚é–“å¾…æ©Ÿ
                        wait_time = 30 + (self.rate_limit_count * 10)
                        logger.info(f"â¸ï¸ {wait_time}ç§’å¾…æ©Ÿã—ã¾ã™...")
                        await asyncio.sleep(wait_time)
                        return None
                    
                    elif response.status == 404:
                        return None
                    
                    else:
                        logger.warning(f"HTTPã‚¨ãƒ©ãƒ¼ {response.status}: {url}")
                        return None
                        
            except asyncio.TimeoutError:
                logger.warning(f"ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {url}")
                self.errors_count += 1
                return None
            except Exception as e:
                logger.error(f"å–å¾—ã‚¨ãƒ©ãƒ¼ {url}: {e}")
                self.errors_count += 1
                return None
    
    async def scrape_list_page(self, url: str) -> List[str]:
        """ãƒªã‚¹ãƒˆãƒšãƒ¼ã‚¸ã‹ã‚‰åº—èˆ—URLã‚’æŠ½å‡º"""
        html = await self.fetch_with_adaptive_delay(url)
        if not html:
            return []
        
        soup = BeautifulSoup(html, 'html.parser')
        restaurant_urls = []
        
        # è¤‡æ•°ã®ã‚»ãƒ¬ã‚¯ã‚¿ã§åº—èˆ—URLã‚’æ¢ã™
        selectors = [
            'a.list-rst__rst-name-target',
            'h3.list-rst__rst-name a',
            'div.list-rst__wrap a[href*="/A"]',
            'a[class*="rst-name"]'
        ]
        
        for selector in selectors:
            links = soup.select(selector)
            for link in links:
                href = link.get('href', '')
                if href and '/A' in href and href not in self.processed_urls:
                    full_url = href if href.startswith('http') else f"https://tabelog.com{href}"
                    restaurant_urls.append(full_url)
        
        return list(set(restaurant_urls))
    
    async def scrape_restaurant_detail(self, url: str) -> Optional[Dict]:
        """åº—èˆ—è©³ç´°ã‚’å–å¾—ï¼ˆv2ç‰ˆã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ä½¿ç”¨ï¼‰"""
        if url in self.processed_urls:
            return None
        
        html = await self.fetch_with_adaptive_delay(url)
        if not html:
            return None
        
        soup = BeautifulSoup(html, 'html.parser')
        
        # åº—åå–å¾—
        shop_name = self._extract_shop_name(soup)
        if not shop_name:
            return None
        
        # åŸºæœ¬æƒ…å ±ã‚’å–å¾—
        info = {
            'shop_name': shop_name,
            'url': url,
            'phone': self._extract_phone(soup),
            'address': self._extract_address(soup),
            'genre': self._extract_genre(soup),
            'station': self._extract_station(soup),
            'open_time': self._extract_open_time(soup),
            'source': 'é£Ÿã¹ãƒ­ã‚°',
            'scraped_at': datetime.now().isoformat()
        }
        
        self.processed_urls.add(url)
        return info
    
    def _extract_shop_name(self, soup: BeautifulSoup) -> str:
        """åº—åã‚’æŠ½å‡º"""
        selectors = [
            'h2.display-name span',
            'h2.display-name',
            'h1.display-name span',
            'h1.display-name',
            'div.rdheader-info__name span',
            'div.rdheader-info__name',
            '.rstinfo-table__name'
        ]
        
        for selector in selectors:
            elem = soup.select_one(selector)
            if elem:
                text = elem.get_text(strip=True)
                text = re.sub(r'\s*\([^)]+\)\s*$', '', text)
                text = re.sub(r'\s+', ' ', text)
                if text and text != 'é£Ÿã¹ãƒ­ã‚°':
                    return text.strip()
        
        return ""
    
    def _extract_address(self, soup: BeautifulSoup) -> str:
        """ä½æ‰€ã‚’æŠ½å‡º"""
        selectors = [
            'p.rstinfo-table__address',
            'span.rstinfo-table__address',
            'div.rstinfo-table__address',
            'p.rstdtl-side-address__text'
        ]
        
        for selector in selectors:
            elem = soup.select_one(selector)
            if elem:
                address = elem.get_text(strip=True)
                address = re.sub(r'ã€’\d{3}-\d{4}\s*', '', address)
                if address:
                    return address
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰æ¤œç´¢
        for th in soup.find_all(['th', 'td'], string=re.compile('ä½æ‰€')):
            next_elem = th.find_next_sibling()
            if next_elem:
                address = next_elem.get_text(strip=True)
                if address:
                    return address
        
        return ""
    
    def _extract_phone(self, soup: BeautifulSoup) -> str:
        """é›»è©±ç•ªå·ã‚’æŠ½å‡º"""
        phone_patterns = [
            r'03-\d{4}-\d{4}',
            r'0\d{2,3}-\d{3,4}-\d{4}',
            r'0\d{9,10}',
            r'\d{2,4}-\d{2,4}-\d{4}'
        ]
        
        selectors = [
            'span.rstinfo-table__tel-num',
            'p.rstinfo-table__tel',
            'div.rstinfo-table__tel'
        ]
        
        for selector in selectors:
            elem = soup.select_one(selector)
            if elem:
                text = elem.get_text(strip=True)
                for pattern in phone_patterns:
                    match = re.search(pattern, text)
                    if match:
                        return match.group()
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰æ¤œç´¢
        for th in soup.find_all(['th', 'td'], string=re.compile('é›»è©±ç•ªå·')):
            next_elem = th.find_next_sibling()
            if next_elem:
                text = next_elem.get_text(strip=True)
                for pattern in phone_patterns:
                    match = re.search(pattern, text)
                    if match:
                        return match.group()
        
        return ""
    
    def _extract_genre(self, soup: BeautifulSoup) -> str:
        """ã‚¸ãƒ£ãƒ³ãƒ«ã‚’æŠ½å‡º"""
        # ã‚¸ãƒ£ãƒ³ãƒ«å°‚ç”¨ã‚»ãƒ¬ã‚¯ã‚¿ã‚’å„ªå…ˆ
        priority_selectors = [
            '.rstinfo-table__genre',
            'span[property="v:category"]'
        ]
        
        for selector in priority_selectors:
            elem = soup.select_one(selector)
            if elem:
                genre = elem.get_text(strip=True)
                if genre and genre != 'é£²é£Ÿåº—':
                    return genre
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰æ¤œç´¢
        for th in soup.find_all(['th', 'td'], string=re.compile('ã‚¸ãƒ£ãƒ³ãƒ«')):
            next_elem = th.find_next_sibling()
            if next_elem:
                genre = next_elem.get_text(strip=True)
                if genre:
                    return genre
        
        # ä¸€èˆ¬çš„ãªã‚»ãƒ¬ã‚¯ã‚¿ï¼ˆé§…åã‚’é™¤å¤–ï¼‰
        for elem in soup.select('span.linktree__parent-target-text'):
            text = elem.get_text(strip=True)
            if text and 'é§…' not in text and not text.endswith('ç·š'):
                genre_keywords = ['æ–™ç†', 'ç„¼', 'é‹', 'å¯¿å¸', 'é®¨', 'ãã°', 'ã†ã©ã‚“', 'ãƒ©ãƒ¼ãƒ¡ãƒ³',
                                'ã‚«ãƒ¬ãƒ¼', 'ã‚¤ã‚¿ãƒªã‚¢ãƒ³', 'ãƒ•ãƒ¬ãƒ³ãƒ', 'ä¸­è¯', 'å’Œé£Ÿ', 'æ´‹é£Ÿ',
                                'ã‚«ãƒ•ã‚§', 'ãƒãƒ¼', 'å±…é…’å±‹', 'é£Ÿå ‚', 'ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³']
                if any(keyword in text for keyword in genre_keywords):
                    return text
        
        return ""
    
    def _extract_station(self, soup: BeautifulSoup) -> str:
        """æœ€å¯„ã‚Šé§…ã‚’æŠ½å‡º"""
        selectors = [
            'span.linktree__parent-target-text:contains("é§…")',
            '.rstinfo-table__access',
            'dl.rdheader-subinfo__item--station'
        ]
        
        for selector in selectors:
            if ':contains' in selector:
                for elem in soup.select('span.linktree__parent-target-text'):
                    text = elem.get_text(strip=True)
                    if 'é§…' in text:
                        return text
            else:
                elem = soup.select_one(selector)
                if elem:
                    station = elem.get_text(strip=True)
                    station = station.split('ã€')[0].split('ã‹ã‚‰')[0]
                    if station:
                        return station
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰æ¤œç´¢
        for keyword in ['äº¤é€šæ‰‹æ®µ', 'æœ€å¯„ã‚Šé§…', 'æœ€å¯„é§…']:
            for th in soup.find_all(['th', 'td'], string=re.compile(keyword)):
                next_elem = th.find_next_sibling()
                if next_elem:
                    station = next_elem.get_text(strip=True)
                    station = station.split('ã€')[0].split('ã‹ã‚‰')[0]
                    if station:
                        return station
        
        return ""
    
    def _extract_open_time(self, soup: BeautifulSoup) -> str:
        """å–¶æ¥­æ™‚é–“ã‚’æŠ½å‡º"""
        selectors = [
            'p.rstinfo-table__open-hours',
            '.rstinfo-table__open-hours',
            'dl.rdheader-subinfo__item--open-hours'
        ]
        
        for selector in selectors:
            elem = soup.select_one(selector)
            if elem:
                hours = elem.get_text(strip=True)
                hours = re.sub(r'\s+', ' ', hours)
                if hours:
                    return hours
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰æ¤œç´¢
        for th in soup.find_all(['th', 'td'], string=re.compile('å–¶æ¥­æ™‚é–“')):
            next_elem = th.find_next_sibling()
            if next_elem:
                hours = next_elem.get_text(strip=True)
                hours = re.sub(r'\s+', ' ', hours)
                if hours:
                    return hours
        
        return ""
    
    async def scrape_restaurants_smart(self, target_count: int = 50000):
        """ã‚¹ãƒãƒ¼ãƒˆãªå¤§é‡åé›†"""
        logger.info(f"ğŸ¯ ã‚¹ãƒãƒ¼ãƒˆãƒ¢ãƒ¼ãƒ‰èµ·å‹•: ç›®æ¨™ {target_count}ä»¶")
        logger.info(f"ğŸ“Š åˆæœŸé…å»¶: {self.current_delay:.1f}ç§’")
        
        # æ—¢å­˜ã®çµæœæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        existing_count = 0
        for result_file in self.results_dir.glob("results_*.json"):
            with open(result_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                existing_count += len(data)
        
        if existing_count >= target_count:
            logger.info(f"æ—¢ã«ç›®æ¨™æ•°ã‚’é”æˆã—ã¦ã„ã¾ã™: {existing_count}ä»¶")
            return
        
        remaining_count = target_count - existing_count
        logger.info(f"ğŸ“ˆ è¿½åŠ åé›†å¿…è¦æ•°: {remaining_count}ä»¶")
        
        # ã‚¨ãƒªã‚¢ã”ã¨ã«æ®µéšçš„ã«åé›†
        all_restaurant_urls = []
        
        for area_name, area_url in self.tokyo_main_areas.items():
            if len(all_restaurant_urls) >= remaining_count * 1.2:  # 20%ä½™åˆ†ã«åé›†
                break
            
            logger.info(f"ğŸ™ï¸ {area_name}ã®ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹")
            
            # å„ã‚¨ãƒªã‚¢ã§æœ€å¤§60ãƒšãƒ¼ã‚¸ã¾ã§åé›†
            area_urls = []
            for page in range(1, 61):
                page_url = f"{area_url}rstLst/{page}/"
                area_urls.append(page_url)
            
            # ãƒãƒƒãƒå‡¦ç†ã§URLã‚’åé›†
            for i in range(0, len(area_urls), 10):
                batch_urls = area_urls[i:i+10]
                tasks = [self.scrape_list_page(url) for url in batch_urls]
                batch_results = await asyncio.gather(*tasks, return_exceptions=True)
                
                for result in batch_results:
                    if isinstance(result, list):
                        all_restaurant_urls.extend(result)
                
                logger.info(f"  åé›†URLæ•°: {len(all_restaurant_urls)}")
                
                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãŒå¤šã„å ´åˆã¯æ¬¡ã®ã‚¨ãƒªã‚¢ã¸
                if self.rate_limit_count > 5:
                    logger.warning(f"ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãŒå¤šã„ãŸã‚ã€æ¬¡ã®ã‚¨ãƒªã‚¢ã¸ç§»å‹•ã—ã¾ã™")
                    self.rate_limit_count = 0
                    await asyncio.sleep(60)  # ã‚¨ãƒªã‚¢åˆ‡ã‚Šæ›¿ãˆæ™‚ã¯é•·ã‚ã«å¾…æ©Ÿ
                    break
        
        # é‡è¤‡ã‚’é™¤å»
        all_restaurant_urls = list(set(all_restaurant_urls) - self.processed_urls)
        logger.info(f"ğŸ“‹ åé›†ã—ãŸãƒ¦ãƒ‹ãƒ¼ã‚¯URLæ•°: {len(all_restaurant_urls)}")
        
        # è©³ç´°æƒ…å ±ã‚’å–å¾—
        collected_count = 0
        for i in range(0, len(all_restaurant_urls), 10):
            if collected_count >= remaining_count:
                break
            
            batch_urls = all_restaurant_urls[i:i+10]
            tasks = [self.scrape_restaurant_detail(url) for url in batch_urls]
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for result in batch_results:
                if isinstance(result, dict) and result:
                    self.results.append(result)
                    collected_count += 1
                    
                    if collected_count % 100 == 0:
                        logger.info(f"âœ… åé›†æ¸ˆã¿: {collected_count}/{remaining_count}ä»¶ (é…å»¶: {self.current_delay:.1f}ç§’)")
                    
                    if collected_count % 1000 == 0:
                        self.save_progress()
            
            # å®šæœŸçš„ã«é•·ã‚ã®ä¼‘æ†©
            if i % 100 == 0 and i > 0:
                logger.info(f"â¸ï¸ å®šæœŸä¼‘æ†©: 30ç§’")
                await asyncio.sleep(30)
        
        # æœ€çµ‚ä¿å­˜
        if self.results:
            self.save_progress()
        
        # çµ±è¨ˆæƒ…å ±
        total_collected = existing_count + collected_count
        logger.info(f"ğŸ‰ åé›†å®Œäº†: æ–°è¦ {collected_count}ä»¶ã€åˆè¨ˆ {total_collected}ä»¶")
        logger.info(f"ğŸ“Š æœ€çµ‚é…å»¶: {self.current_delay:.1f}ç§’")
        logger.info(f"âŒ ã‚¨ãƒ©ãƒ¼æ•°: {self.errors_count}")


async def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    target_count = int(sys.argv[1]) if len(sys.argv) > 1 else 50000
    
    async with TabelogSmartScraperV4(max_concurrent=20) as scraper:
        await scraper.scrape_restaurants_smart(target_count)
        
        # å…¨çµæœã‚’çµ±åˆã—ã¦Excelã«å‡ºåŠ›
        from restaurant_data_integrator import RestaurantDataIntegrator
        integrator = RestaurantDataIntegrator()
        
        # å…¨çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        all_results = []
        results_dir = Path("output") / "smart_results_v4"
        
        for result_file in sorted(results_dir.glob("results_*.json")):
            with open(result_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                all_results.extend(data)
        
        if not all_results:
            logger.warning("åé›†ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        # é‡è¤‡é™¤å»
        unique_results = []
        seen_urls = set()
        for r in all_results:
            if r['url'] not in seen_urls:
                unique_results.append(r)
                seen_urls.add(r['url'])
        
        logger.info(f"çµ±åˆçµæœ: {len(unique_results)}ä»¶ï¼ˆé‡è¤‡é™¤å»å¾Œï¼‰")
        
        # Excelå‡ºåŠ›
        integrator.add_restaurants(unique_results)
        integrator.remove_duplicates()
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_file = f'output/æ±äº¬éƒ½_é£²é£Ÿåº—ãƒªã‚¹ãƒˆ_SMART_{len(unique_results)}ä»¶_{timestamp}.xlsx'
        integrator.create_excel_report(output_file)
        
        logger.info(f"âœ… Excelãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå®Œäº†: {output_file}")
        
        # ãƒ‡ãƒ¼ã‚¿å“è³ªçµ±è¨ˆ
        with_phone = sum(1 for r in unique_results if r.get('phone'))
        with_address = sum(1 for r in unique_results if r.get('address'))
        with_genre = sum(1 for r in unique_results if r.get('genre'))
        with_station = sum(1 for r in unique_results if r.get('station'))
        
        logger.info(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿å“è³ª:")
        logger.info(f"  - é›»è©±ç•ªå·ã‚ã‚Š: {with_phone}ä»¶ ({with_phone/len(unique_results)*100:.1f}%)")
        logger.info(f"  - ä½æ‰€ã‚ã‚Š: {with_address}ä»¶ ({with_address/len(unique_results)*100:.1f}%)")
        logger.info(f"  - ã‚¸ãƒ£ãƒ³ãƒ«ã‚ã‚Š: {with_genre}ä»¶ ({with_genre/len(unique_results)*100:.1f}%)")
        logger.info(f"  - æœ€å¯„ã‚Šé§…ã‚ã‚Š: {with_station}ä»¶ ({with_station/len(unique_results)*100:.1f}%)")


if __name__ == "__main__":
    asyncio.run(main())