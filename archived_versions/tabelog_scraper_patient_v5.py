#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
é£Ÿã¹ãƒ­ã‚°ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ å¿è€ç‰ˆ V5
- è¶…ä½é€Ÿã ãŒç¢ºå®Ÿãªåé›†
- ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’å®Œå…¨ã«å›é¿
- é•·æ™‚é–“å®Ÿè¡Œå‰æ
"""

import asyncio
import aiohttp
import json
import re
import random
import time
import os
import sys
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Set, Tuple
from bs4 import BeautifulSoup
from pathlib import Path
import logging
from aiohttp import ClientTimeout, TCPConnector

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('tabelog_patient_v5.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class TabelogPatientScraperV5:
    """å¿è€å¼·ã„é£Ÿã¹ãƒ­ã‚°ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.max_concurrent = 1  # åŒæ™‚æ¥ç¶šæ•°ã‚’1ã«åˆ¶é™
        self.semaphore = asyncio.Semaphore(self.max_concurrent)
        self.session = None
        
        # å›ºå®šã®é•·ã„é…å»¶
        self.base_delay = 15.0  # åŸºæœ¬é…å»¶15ç§’
        self.random_delay_range = (5.0, 10.0)  # è¿½åŠ ãƒ©ãƒ³ãƒ€ãƒ é…å»¶
        
        # é€²æ—ç®¡ç†
        self.processed_urls: Set[str] = set()
        self.results: List[Dict] = []
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        self.cache_dir = Path("cache")
        self.cache_dir.mkdir(exist_ok=True)
        self.progress_file = self.cache_dir / "patient_progress_v5.json"
        self.results_file = self.cache_dir / "patient_results_v5.json"
        
        # æ±äº¬ã®å…¨ã‚¨ãƒªã‚¢URL
        self.area_base_urls = []
        for i in range(1301, 1325):  # A1301ã‹ã‚‰A1324ã¾ã§
            self.area_base_urls.append(f"https://tabelog.com/tokyo/A{i}/")
        
    async def __aenter__(self):
        """éåŒæœŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£å…¥å£"""
        timeout = ClientTimeout(total=60, connect=20, sock_read=20)
        self.session = aiohttp.ClientSession(
            timeout=timeout,
            headers=self._get_headers()
        )
        
        # å‰å›ã®é€²æ—ã‚’èª­ã¿è¾¼ã‚€
        self.load_progress()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """éåŒæœŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£å‡ºå£"""
        if self.session:
            await self.session.close()
    
    def _get_headers(self) -> Dict[str, str]:
        """ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ˜ãƒƒãƒ€ãƒ¼ã‚’å–å¾—"""
        return {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja-JP,ja;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache'
        }
    
    def load_progress(self):
        """å‰å›ã®é€²æ—ã‚’èª­ã¿è¾¼ã‚€"""
        try:
            if self.progress_file.exists():
                with open(self.progress_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.processed_urls = set(data.get('processed_urls', []))
                    logger.info(f"å‰å›ã®é€²æ—ã‚’èª­ã¿è¾¼ã¿: {len(self.processed_urls)}ä»¶å‡¦ç†æ¸ˆã¿")
            
            if self.results_file.exists():
                with open(self.results_file, 'r', encoding='utf-8') as f:
                    self.results = json.load(f)
                    logger.info(f"å‰å›ã®çµæœã‚’èª­ã¿è¾¼ã¿: {len(self.results)}ä»¶")
                
        except Exception as e:
            logger.error(f"é€²æ—èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    def save_progress(self):
        """é€²æ—ã‚’ä¿å­˜"""
        try:
            with open(self.progress_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'processed_urls': list(self.processed_urls),
                    'total_processed': len(self.processed_urls),
                    'timestamp': datetime.now().isoformat()
                }, f, ensure_ascii=False, indent=2)
            
            with open(self.results_file, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, ensure_ascii=False, indent=2)
                    
        except Exception as e:
            logger.error(f"é€²æ—ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    async def patient_fetch(self, url: str) -> Optional[str]:
        """å¿è€å¼·ããƒšãƒ¼ã‚¸ã‚’å–å¾—"""
        # é•·ã„é…å»¶
        total_delay = self.base_delay + random.uniform(*self.random_delay_range)
        logger.info(f"â³ {total_delay:.1f}ç§’å¾…æ©Ÿä¸­...")
        await asyncio.sleep(total_delay)
        
        try:
            async with self.session.get(url) as response:
                if response.status == 200:
                    logger.info(f"âœ… å–å¾—æˆåŠŸ: {url}")
                    return await response.text()
                elif response.status == 429:
                    logger.warning(f"âš ï¸ ãƒ¬ãƒ¼ãƒˆåˆ¶é™æ¤œå‡ºã€‚è¿½åŠ ã§60ç§’å¾…æ©Ÿ")
                    await asyncio.sleep(60)
                    return None
                else:
                    logger.warning(f"HTTPã‚¨ãƒ©ãƒ¼ {response.status}: {url}")
                    return None
                    
        except Exception as e:
            logger.error(f"å–å¾—ã‚¨ãƒ©ãƒ¼ {url}: {e}")
            return None
    
    async def scrape_list_page(self, url: str) -> List[str]:
        """ãƒªã‚¹ãƒˆãƒšãƒ¼ã‚¸ã‹ã‚‰åº—èˆ—URLã‚’æŠ½å‡º"""
        html = await self.patient_fetch(url)
        if not html:
            return []
        
        soup = BeautifulSoup(html, 'html.parser')
        restaurant_urls = []
        
        # ã‚·ãƒ³ãƒ—ãƒ«ãªã‚»ãƒ¬ã‚¯ã‚¿ã®ã¿ä½¿ç”¨
        links = soup.select('a[href*="/A"][href*="/A"]')
        for link in links:
            href = link.get('href', '')
            if re.match(r'.*/A\d+/A\d+/\d+/$', href):
                full_url = f"https://tabelog.com{href}" if not href.startswith('http') else href
                if full_url not in self.processed_urls:
                    restaurant_urls.append(full_url)
        
        return list(set(restaurant_urls))
    
    async def scrape_restaurant_detail(self, url: str) -> Optional[Dict]:
        """åº—èˆ—è©³ç´°ã‚’å–å¾—"""
        if url in self.processed_urls:
            return None
        
        html = await self.patient_fetch(url)
        if not html:
            return None
        
        soup = BeautifulSoup(html, 'html.parser')
        
        # åº—å
        shop_name = ""
        name_elem = soup.select_one('h2.display-name span') or soup.select_one('h2.display-name')
        if name_elem:
            shop_name = name_elem.get_text(strip=True)
            shop_name = re.sub(r'\s*\([^)]+\)\s*$', '', shop_name)
        
        if not shop_name:
            return None
        
        # åŸºæœ¬æƒ…å ±
        info = {
            'shop_name': shop_name,
            'url': url,
            'phone': "",
            'address': "",
            'genre': "",
            'station': "",
            'open_time': "",
            'source': 'é£Ÿã¹ãƒ­ã‚°',
            'scraped_at': datetime.now().isoformat()
        }
        
        # é›»è©±ç•ªå·
        phone_elem = soup.select_one('span.rstinfo-table__tel-num')
        if phone_elem:
            phone_match = re.search(r'[\d\-]+', phone_elem.get_text())
            if phone_match:
                info['phone'] = phone_match.group()
        
        # ä½æ‰€
        addr_elem = soup.select_one('p.rstinfo-table__address')
        if addr_elem:
            info['address'] = addr_elem.get_text(strip=True)
        
        # ã‚¸ãƒ£ãƒ³ãƒ«
        for th in soup.find_all(['th', 'td'], string=re.compile('ã‚¸ãƒ£ãƒ³ãƒ«')):
            next_elem = th.find_next_sibling()
            if next_elem:
                info['genre'] = next_elem.get_text(strip=True)
                break
        
        # æœ€å¯„ã‚Šé§…
        for elem in soup.select('span.linktree__parent-target-text'):
            text = elem.get_text(strip=True)
            if 'é§…' in text:
                info['station'] = text
                break
        
        # å–¶æ¥­æ™‚é–“
        hours_elem = soup.select_one('p.rstinfo-table__open-hours')
        if hours_elem:
            info['open_time'] = re.sub(r'\s+', ' ', hours_elem.get_text(strip=True))
        
        self.processed_urls.add(url)
        return info
    
    async def scrape_restaurants_patient(self, target_count: int = 50000):
        """å¿è€å¼·ãå¤§é‡åé›†"""
        logger.info(f"ğŸ¢ å¿è€ãƒ¢ãƒ¼ãƒ‰èµ·å‹•: ç›®æ¨™ {target_count}ä»¶")
        logger.info(f"â±ï¸ åŸºæœ¬é…å»¶: {self.base_delay}ç§’ + ãƒ©ãƒ³ãƒ€ãƒ {self.random_delay_range}ç§’")
        
        current_count = len(self.results)
        if current_count >= target_count:
            logger.info(f"æ—¢ã«ç›®æ¨™æ•°ã‚’é”æˆ: {current_count}ä»¶")
            return
        
        # å„ã‚¨ãƒªã‚¢ã‚’é †ç•ªã«å‡¦ç†
        for area_url in self.area_base_urls:
            if len(self.results) >= target_count:
                break
            
            area_code = area_url.split('/')[-2]
            logger.info(f"\nğŸ“ ã‚¨ãƒªã‚¢ {area_code} ã®å‡¦ç†é–‹å§‹")
            
            # å„ã‚¨ãƒªã‚¢ã§1-30ãƒšãƒ¼ã‚¸ã¾ã§
            for page in range(1, 31):
                if len(self.results) >= target_count:
                    break
                
                list_url = f"{area_url}rstLst/{page}/"
                logger.info(f"\nğŸ“„ ãƒšãƒ¼ã‚¸ {page}/30 ã‚’å‡¦ç†ä¸­...")
                
                # ãƒªã‚¹ãƒˆãƒšãƒ¼ã‚¸ã‹ã‚‰åº—èˆ—URLå–å¾—
                restaurant_urls = await self.scrape_list_page(list_url)
                logger.info(f"  â†’ {len(restaurant_urls)}ä»¶ã®æ–°è¦åº—èˆ—URLç™ºè¦‹")
                
                # å„åº—èˆ—ã®è©³ç´°ã‚’å–å¾—
                for i, rest_url in enumerate(restaurant_urls):
                    if len(self.results) >= target_count:
                        break
                    
                    logger.info(f"\nğŸ´ åº—èˆ— {i+1}/{len(restaurant_urls)} ã‚’å‡¦ç†ä¸­...")
                    result = await self.scrape_restaurant_detail(rest_url)
                    
                    if result:
                        self.results.append(result)
                        current_count = len(self.results)
                        logger.info(f"  âœ… åé›†æˆåŠŸ: {result['shop_name']} (åˆè¨ˆ: {current_count}ä»¶)")
                        
                        # 10ä»¶ã”ã¨ã«ä¿å­˜
                        if current_count % 10 == 0:
                            self.save_progress()
                            logger.info(f"ğŸ’¾ é€²æ—ä¿å­˜: {current_count}ä»¶")
                        
                        # 100ä»¶ã”ã¨ã«çµ±è¨ˆè¡¨ç¤º
                        if current_count % 100 == 0:
                            elapsed = (datetime.now() - datetime.fromisoformat(self.results[0]['scraped_at'])).total_seconds()
                            rate = current_count / (elapsed / 3600)  # ä»¶/æ™‚é–“
                            eta_hours = (target_count - current_count) / rate if rate > 0 else 0
                            logger.info(f"\nğŸ“Š çµ±è¨ˆ:")
                            logger.info(f"  åé›†é€Ÿåº¦: {rate:.1f}ä»¶/æ™‚é–“")
                            logger.info(f"  æ¨å®šå®Œäº†æ™‚é–“: {eta_hours:.1f}æ™‚é–“å¾Œ")
            
            # ã‚¨ãƒªã‚¢åˆ‡ã‚Šæ›¿ãˆæ™‚ã¯é•·ã‚ã«ä¼‘æ†©
            logger.info(f"\nâ¸ï¸ ã‚¨ãƒªã‚¢åˆ‡ã‚Šæ›¿ãˆ: 60ç§’ä¼‘æ†©")
            await asyncio.sleep(60)
        
        # æœ€çµ‚ä¿å­˜
        self.save_progress()
        logger.info(f"\nğŸ‰ åé›†å®Œäº†: åˆè¨ˆ {len(self.results)}ä»¶")


async def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    target_count = int(sys.argv[1]) if len(sys.argv) > 1 else 5000  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯5000ä»¶
    
    async with TabelogPatientScraperV5() as scraper:
        await scraper.scrape_restaurants_patient(target_count)
        
        # Excelå‡ºåŠ›
        if scraper.results:
            from restaurant_data_integrator import RestaurantDataIntegrator
            integrator = RestaurantDataIntegrator()
            
            integrator.add_restaurants(scraper.results)
            integrator.remove_duplicates()
            
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_file = f'output/æ±äº¬éƒ½_é£²é£Ÿåº—ãƒªã‚¹ãƒˆ_å¿è€_{len(scraper.results)}ä»¶_{timestamp}.xlsx'
            integrator.create_excel_report(output_file)
            
            logger.info(f"âœ… Excelãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ: {output_file}")
            
            # ãƒ‡ãƒ¼ã‚¿å“è³ª
            with_phone = sum(1 for r in scraper.results if r.get('phone'))
            with_address = sum(1 for r in scraper.results if r.get('address'))
            logger.info(f"\nğŸ“Š ãƒ‡ãƒ¼ã‚¿å“è³ª:")
            logger.info(f"  é›»è©±ç•ªå·: {with_phone}/{len(scraper.results)} ({with_phone/len(scraper.results)*100:.1f}%)")
            logger.info(f"  ä½æ‰€: {with_address}/{len(scraper.results)} ({with_address/len(scraper.results)*100:.1f}%)")


if __name__ == "__main__":
    asyncio.run(main())