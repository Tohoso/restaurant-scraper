#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
éåŒæœŸç‰ˆé£Ÿã¹ãƒ­ã‚°ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ V2
æœ€æ–°ã®é£Ÿã¹ãƒ­ã‚°HTMLæ§‹é€ ã«å¯¾å¿œã—ãŸé«˜é€Ÿã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼
"""

import asyncio
import aiohttp
from aiohttp import ClientTimeout, TCPConnector
from bs4 import BeautifulSoup
import time
import re
from typing import List, Dict, Optional, Set
import logging
from urllib.parse import urljoin, urlparse
import random
from datetime import datetime
import json
import os
from pathlib import Path

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TabelogScraperAsyncV2:
    """éåŒæœŸç‰ˆé£Ÿã¹ãƒ­ã‚°ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ V2"""
    
    def __init__(self, max_concurrent: int = 10, delay_range: tuple = (0.5, 1.0)):
        """
        åˆæœŸåŒ–
        
        Args:
            max_concurrent: æœ€å¤§åŒæ™‚æ¥ç¶šæ•°
            delay_range: ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã®é…å»¶ç¯„å›²ï¼ˆç§’ï¼‰
        """
        self.max_concurrent = max_concurrent
        self.delay_range = delay_range
        self.base_url = "https://tabelog.com"
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.session = None
        self.progress_file = Path("cache/scraping_progress_v2.json")
        self.results_file = Path("cache/partial_results_v2.json")
        self.processed_urls: Set[str] = set()
        self.results: List[Dict] = []
        
    async def __aenter__(self):
        """éåŒæœŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®é–‹å§‹"""
        timeout = ClientTimeout(total=30, connect=10, sock_read=10)
        connector = TCPConnector(limit=self.max_concurrent, force_close=True)
        self.session = aiohttp.ClientSession(
            timeout=timeout,
            connector=connector,
            headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1'
            }
        )
        # é€²æ—ã‚’èª­ã¿è¾¼ã‚€
        self._load_progress()
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """éåŒæœŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®çµ‚äº†"""
        if self.session:
            await self.session.close()
            
    def _load_progress(self):
        """å‰å›ã®é€²æ—ã‚’èª­ã¿è¾¼ã‚€"""
        if self.progress_file.exists():
            try:
                with open(self.progress_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.processed_urls = set(data.get('processed_urls', []))
                    logger.info(f"å‰å›ã®é€²æ—ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {len(self.processed_urls)}ä»¶å‡¦ç†æ¸ˆã¿")
            except Exception as e:
                logger.error(f"é€²æ—èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
                
        if self.results_file.exists():
            try:
                with open(self.results_file, 'r', encoding='utf-8') as f:
                    self.results = json.load(f)
                    logger.info(f"å‰å›ã®çµæœã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {len(self.results)}ä»¶")
            except Exception as e:
                logger.error(f"çµæœèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _save_progress(self):
        """é€²æ—ã‚’ä¿å­˜"""
        try:
            self.progress_file.parent.mkdir(exist_ok=True)
            with open(self.progress_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'processed_urls': list(self.processed_urls),
                    'timestamp': datetime.now().isoformat()
                }, f, ensure_ascii=False, indent=2)
                
            with open(self.results_file, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            logger.error(f"é€²æ—ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def get_area_urls(self) -> Dict[str, str]:
        """åœ°åŸŸåˆ¥URLã‚’å–å¾—"""
        return {
            'æ±äº¬éƒ½': 'https://tabelog.com/tokyo/',
            'å¤§é˜ªåºœ': 'https://tabelog.com/osaka/',
            'ç¥å¥ˆå·çœŒ': 'https://tabelog.com/kanagawa/',
            'æ„›çŸ¥çœŒ': 'https://tabelog.com/aichi/',
            'ç¦å²¡çœŒ': 'https://tabelog.com/fukuoka/',
            'åŒ—æµ·é“': 'https://tabelog.com/hokkaido/',
            'äº¬éƒ½åºœ': 'https://tabelog.com/kyoto/',
            'å…µåº«çœŒ': 'https://tabelog.com/hyogo/',
            'åŸ¼ç‰çœŒ': 'https://tabelog.com/saitama/',
            'åƒè‘‰çœŒ': 'https://tabelog.com/chiba/'
        }
    
    async def fetch_page(self, url: str) -> Optional[str]:
        """ãƒšãƒ¼ã‚¸ã‚’éåŒæœŸã§å–å¾—"""
        async with self.semaphore:
            try:
                # ãƒ©ãƒ³ãƒ€ãƒ ãªé…å»¶
                delay = random.uniform(*self.delay_range)
                await asyncio.sleep(delay)
                
                async with self.session.get(url) as response:
                    if response.status == 200:
                        return await response.text()
                    else:
                        logger.warning(f"HTTPã‚¨ãƒ©ãƒ¼ {response.status}: {url}")
                        return None
                        
            except asyncio.TimeoutError:
                logger.error(f"ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {url}")
                return None
            except Exception as e:
                logger.error(f"å–å¾—ã‚¨ãƒ©ãƒ¼ {url}: {e}")
                return None
    
    async def scrape_restaurant_list_async(self, area_url: str, max_pages: int = 5) -> List[str]:
        """
        ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³ãƒªã‚¹ãƒˆãƒšãƒ¼ã‚¸ã‹ã‚‰å€‹åˆ¥åº—èˆ—URLã‚’éåŒæœŸã§å–å¾—
        """
        restaurant_urls = []
        tasks = []
        
        # ãƒšãƒ¼ã‚¸URLã‚’ç”Ÿæˆ
        page_urls = [area_url]
        for page in range(2, max_pages + 1):
            page_urls.append(f"{area_url}rstLst/{page}/")
        
        # éåŒæœŸã§ãƒšãƒ¼ã‚¸ã‚’å–å¾—
        for page_url in page_urls:
            task = self.fetch_page(page_url)
            tasks.append(task)
        
        pages = await asyncio.gather(*tasks)
        
        # å„ãƒšãƒ¼ã‚¸ã‹ã‚‰åº—èˆ—URLã‚’æŠ½å‡º
        for i, html in enumerate(pages):
            if html:
                soup = BeautifulSoup(html, 'html.parser')
                links = self._extract_restaurant_links(soup)
                
                for href in links:
                    full_url = urljoin(self.base_url, href)
                    if full_url not in restaurant_urls:
                        restaurant_urls.append(full_url)
                
                logger.info(f"ãƒšãƒ¼ã‚¸ {i+1}: {len(links)}ä»¶ã®åº—èˆ—URLå–å¾—")
        
        logger.info(f"åˆè¨ˆ {len(restaurant_urls)} ä»¶ã®åº—èˆ—URLå–å¾—")
        return restaurant_urls
    
    def _extract_restaurant_links(self, soup: BeautifulSoup) -> List[str]:
        """ãƒšãƒ¼ã‚¸ã‹ã‚‰åº—èˆ—ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º"""
        links = []
        
        # è¤‡æ•°ã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦ã™
        selectors = [
            'a.list-rst__rst-name-target',
            'a.js-restaurant-link',
            'h3.list-rst__rst-name a',
            'div.list-rst__rst-name a',
            'a[href*="/A"][href*="/A"][href*="/"]'
        ]
        
        for selector in selectors:
            elements = soup.select(selector)
            if elements:
                links = [elem.get('href') for elem in elements if elem.get('href')]
                break
        
        # hrefãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã‚‚æ¤œç´¢
        if not links:
            all_links = soup.find_all('a', href=True)
            links = [link.get('href') for link in all_links 
                    if re.match(r'/[^/]+/A\d+/A\d+/\d+/', link.get('href', ''))]
        
        return links
    
    async def scrape_restaurant_detail_async(self, restaurant_url: str) -> Optional[Dict]:
        """å€‹åˆ¥åº—èˆ—ãƒšãƒ¼ã‚¸ã‹ã‚‰è©³ç´°æƒ…å ±ã‚’éåŒæœŸã§å–å¾—"""
        
        # æ—¢ã«å‡¦ç†æ¸ˆã¿ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        if restaurant_url in self.processed_urls:
            return None
            
        try:
            # ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã‚’å–å¾—
            main_html = await self.fetch_page(restaurant_url)
            
            if not main_html:
                return None
            
            soup = BeautifulSoup(main_html, 'html.parser')
            
            # åº—åå–å¾—ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
            shop_name = self._extract_shop_name_v2(soup)
            if not shop_name:
                logger.warning(f"åº—åãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ: {restaurant_url}")
                return None
            
            # åŸºæœ¬æƒ…å ±ã‚’å–å¾—ï¼ˆæ–°ã—ã„ã‚»ãƒ¬ã‚¯ã‚¿ï¼‰
            address = self._extract_address_v2(soup)
            phone = self._extract_phone_v2(soup)
            genre = self._extract_genre_v2(soup)
            station = self._extract_station_v2(soup)
            open_time = self._extract_open_time_v2(soup)
            
            # æƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã¯åœ°å›³ãƒšãƒ¼ã‚¸ã‚‚ç¢ºèª
            if not address or not phone:
                map_url = restaurant_url.rstrip('/') + '/dtlmap/'
                map_html = await self.fetch_page(map_url)
                if map_html:
                    map_soup = BeautifulSoup(map_html, 'html.parser')
                    if not phone:
                        phone = self._extract_phone_from_map(map_soup)
                    if not address:
                        address = self._extract_address_from_map(map_soup)
            
            restaurant_info = {
                'shop_name': shop_name,
                'phone': phone,
                'address': address,
                'genre': genre,
                'station': station,
                'open_time': open_time,
                'url': restaurant_url,
                'source': 'é£Ÿã¹ãƒ­ã‚°',
                'scraped_at': datetime.now().isoformat()
            }
            
            # å‡¦ç†æ¸ˆã¿ã¨ã—ã¦è¨˜éŒ²
            self.processed_urls.add(restaurant_url)
            self.results.append(restaurant_info)
            
            # å®šæœŸçš„ã«é€²æ—ã‚’ä¿å­˜ï¼ˆ10ä»¶ã”ã¨ï¼‰
            if len(self.results) % 10 == 0:
                self._save_progress()
                logger.info(f"é€²æ—ä¿å­˜: {len(self.results)}ä»¶å®Œäº†")
            
            logger.info(f"âœ… åº—èˆ—æƒ…å ±å–å¾—æˆåŠŸ [{len(self.results)}ä»¶ç›®]: {shop_name}")
            return restaurant_info
            
        except Exception as e:
            logger.error(f"åº—èˆ—è©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼ {restaurant_url}: {e}")
            return None
    
    def _extract_shop_name_v2(self, soup: BeautifulSoup) -> str:
        """åº—åã‚’æŠ½å‡ºï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        # å„ªå…ˆé †ä½ã®é«˜ã„é †ã«ã‚»ãƒ¬ã‚¯ã‚¿ã‚’è©¦ã™
        selectors = [
            'h2.display-name span',
            'h2.display-name',
            'h1.display-name span',
            'h1.display-name',
            'div.rdheader-info__name span',
            'div.rdheader-info__name',
            '.rstinfo-table__name',
            'h1',
            'title'
        ]
        
        for selector in selectors:
            elem = soup.select_one(selector)
            if elem:
                text = elem.get_text(strip=True)
                # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                text = re.sub(r'\s*\([^)]+\)\s*$', '', text)  # æ‹¬å¼§å†…ã®èª­ã¿ä»®åã‚’å‰Šé™¤
                text = re.sub(r'\s+', ' ', text)  # ä½™åˆ†ãªç©ºç™½ã‚’å‰Šé™¤
                if text and text != 'é£Ÿã¹ãƒ­ã‚°':
                    return text.strip()
        
        return ""
    
    def _extract_address_v2(self, soup: BeautifulSoup) -> str:
        """ä½æ‰€ã‚’æŠ½å‡ºï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        # å„ªå…ˆé †ä½ã®é«˜ã„é †ã«ã‚»ãƒ¬ã‚¯ã‚¿ã‚’è©¦ã™
        selectors = [
            'p.rstinfo-table__address',
            'span.rstinfo-table__address',
            'td:contains("ä½æ‰€") + td',
            'th:contains("ä½æ‰€") + td',
            'div.rstinfo-table__address',
            'p.rstdtl-side-address__text',
            '.rdheader-subinfo__item-text'
        ]
        
        for selector in selectors:
            if ':contains' in selector:
                # :contains ã‚»ãƒ¬ã‚¯ã‚¿ã®å‡¦ç†
                elements = soup.find_all(text=re.compile('ä½æ‰€'))
                for elem in elements:
                    parent = elem.parent
                    if parent and parent.name in ['td', 'th']:
                        next_elem = parent.find_next_sibling()
                        if next_elem:
                            address = next_elem.get_text(strip=True)
                            if address:
                                return address
            else:
                elem = soup.select_one(selector)
                if elem:
                    address = elem.get_text(strip=True)
                    # éƒµä¾¿ç•ªå·ã‚„ä¸è¦ãªæ–‡å­—ã‚’å‰Šé™¤
                    address = re.sub(r'ã€’\d{3}-\d{4}\s*', '', address)
                    address = re.sub(r'^\s*ä½æ‰€\s*[:ï¼š]\s*', '', address)
                    if address:
                        return address
        
        return ""
    
    def _extract_phone_v2(self, soup: BeautifulSoup) -> str:
        """é›»è©±ç•ªå·ã‚’æŠ½å‡ºï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        # é›»è©±ç•ªå·ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
        phone_patterns = [
            r'03-\d{4}-\d{4}',
            r'0\d{2,3}-\d{3,4}-\d{4}',
            r'0\d{9,10}',
            r'\d{2,4}-\d{2,4}-\d{4}'
        ]
        
        # å„ªå…ˆé †ä½ã®é«˜ã„é †ã«ã‚»ãƒ¬ã‚¯ã‚¿ã‚’è©¦ã™
        selectors = [
            'span.rstinfo-table__tel-num',
            'p.rstinfo-table__tel',
            'td:contains("é›»è©±ç•ªå·") + td',
            'th:contains("é›»è©±ç•ªå·") + td',
            'div.rstinfo-table__tel',
            '.rdheader-subinfo__item--tel'
        ]
        
        for selector in selectors:
            if ':contains' in selector:
                # :contains ã‚»ãƒ¬ã‚¯ã‚¿ã®å‡¦ç†
                elements = soup.find_all(text=re.compile('é›»è©±ç•ªå·'))
                for elem in elements:
                    parent = elem.parent
                    if parent and parent.name in ['td', 'th']:
                        next_elem = parent.find_next_sibling()
                        if next_elem:
                            text = next_elem.get_text(strip=True)
                            for pattern in phone_patterns:
                                match = re.search(pattern, text)
                                if match:
                                    return match.group()
            else:
                elem = soup.select_one(selector)
                if elem:
                    text = elem.get_text(strip=True)
                    for pattern in phone_patterns:
                        match = re.search(pattern, text)
                        if match:
                            return match.group()
        
        return ""
    
    def _extract_genre_v2(self, soup: BeautifulSoup) -> str:
        """ã‚¸ãƒ£ãƒ³ãƒ«ã‚’æŠ½å‡ºï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        # ã¾ãšã€æ˜ç¢ºã«ã‚¸ãƒ£ãƒ³ãƒ«ã¨ãƒ©ãƒ™ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‚»ãƒ¬ã‚¯ã‚¿ã‚’å„ªå…ˆ
        priority_selectors = [
            'td:contains("ã‚¸ãƒ£ãƒ³ãƒ«") + td',
            'th:contains("ã‚¸ãƒ£ãƒ³ãƒ«") + td',
            '.rstinfo-table__genre',
            'span[property="v:category"]'
        ]
        
        for selector in priority_selectors:
            if ':contains' in selector:
                elements = soup.find_all(text=re.compile('ã‚¸ãƒ£ãƒ³ãƒ«'))
                for elem in elements:
                    parent = elem.parent
                    if parent and parent.name in ['td', 'th']:
                        next_elem = parent.find_next_sibling()
                        if next_elem:
                            genre = next_elem.get_text(strip=True)
                            if genre:
                                return genre
            else:
                elem = soup.select_one(selector)
                if elem:
                    genre = elem.get_text(strip=True)
                    if genre and genre != 'é£²é£Ÿåº—':
                        return genre
        
        # æ¬¡ã«ã€ã‚ˆã‚Šä¸€èˆ¬çš„ãªã‚»ãƒ¬ã‚¯ã‚¿ã‚’è©¦ã™ãŒã€é§…åã‚’é™¤å¤–
        general_selectors = [
            'span.linktree__parent-target-text',
            'div.rdheader-subinfo__item-text'
        ]
        
        for selector in general_selectors:
            elements = soup.select(selector)
            for elem in elements:
                text = elem.get_text(strip=True)
                # é§…åã§ãªã„ã“ã¨ã‚’ç¢ºèªï¼ˆã€Œé§…ã€ãŒå«ã¾ã‚Œã¦ã„ãªã„ã€ã‹ã¤æ–™ç†ã‚¸ãƒ£ãƒ³ãƒ«ã®ç‰¹å¾´ãŒã‚ã‚‹ï¼‰
                if text and 'é§…' not in text and not text.endswith('ç·š'):
                    # ä¸€èˆ¬çš„ãªæ–™ç†ã‚¸ãƒ£ãƒ³ãƒ«ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒã‚§ãƒƒã‚¯
                    genre_keywords = ['æ–™ç†', 'ç„¼', 'é‹', 'å¯¿å¸', 'é®¨', 'ãã°', 'ã†ã©ã‚“', 'ãƒ©ãƒ¼ãƒ¡ãƒ³', 
                                    'ã‚«ãƒ¬ãƒ¼', 'ã‚¤ã‚¿ãƒªã‚¢ãƒ³', 'ãƒ•ãƒ¬ãƒ³ãƒ', 'ä¸­è¯', 'å’Œé£Ÿ', 'æ´‹é£Ÿ', 
                                    'ã‚«ãƒ•ã‚§', 'ãƒãƒ¼', 'å±…é…’å±‹', 'é£Ÿå ‚', 'ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³', 'è‚‰', 'é­š', 
                                    'é‡èœ', 'ä¸²', 'å¤©ã·ã‚‰', 'ä¸¼', 'å®šé£Ÿ']
                    if any(keyword in text for keyword in genre_keywords) or len(text) < 20:
                        return text
        
        return ""
    
    def _extract_station_v2(self, soup: BeautifulSoup) -> str:
        """æœ€å¯„ã‚Šé§…ã‚’æŠ½å‡ºï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        selectors = [
            'span.linktree__parent-target-text:contains("é§…")',
            'td:contains("äº¤é€šæ‰‹æ®µ") + td',
            'th:contains("äº¤é€šæ‰‹æ®µ") + td',
            'td:contains("æœ€å¯„ã‚Šé§…") + td',
            'th:contains("æœ€å¯„ã‚Šé§…") + td',
            '.rstinfo-table__access',
            'dl.rdheader-subinfo__item--station'
        ]
        
        for selector in selectors:
            if ':contains' in selector:
                if 'äº¤é€šæ‰‹æ®µ' in selector or 'æœ€å¯„ã‚Šé§…' in selector:
                    pattern = 'äº¤é€šæ‰‹æ®µ' if 'äº¤é€šæ‰‹æ®µ' in selector else 'æœ€å¯„ã‚Šé§…'
                    elements = soup.find_all(text=re.compile(pattern))
                    for elem in elements:
                        parent = elem.parent
                        if parent and parent.name in ['td', 'th']:
                            next_elem = parent.find_next_sibling()
                            if next_elem:
                                station = next_elem.get_text(strip=True)
                                # æœ€åˆã®é§…åã ã‘æŠ½å‡º
                                station = station.split('ã€')[0].split('ã‹ã‚‰')[0]
                                if station:
                                    return station
                else:
                    elem = soup.select_one(selector)
                    if elem:
                        station = elem.get_text(strip=True)
                        if 'é§…' in station:
                            return station
            else:
                elem = soup.select_one(selector)
                if elem:
                    station = elem.get_text(strip=True)
                    station = station.split('ã€')[0].split('ã‹ã‚‰')[0]
                    if station:
                        return station
        
        return ""
    
    def _extract_open_time_v2(self, soup: BeautifulSoup) -> str:
        """å–¶æ¥­æ™‚é–“ã‚’æŠ½å‡ºï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        selectors = [
            'td:contains("å–¶æ¥­æ™‚é–“") + td',
            'th:contains("å–¶æ¥­æ™‚é–“") + td',
            'p.rstinfo-table__open-hours',
            '.rstinfo-table__open-hours',
            'dl.rdheader-subinfo__item--open-hours'
        ]
        
        for selector in selectors:
            if ':contains' in selector:
                elements = soup.find_all(text=re.compile('å–¶æ¥­æ™‚é–“'))
                for elem in elements:
                    parent = elem.parent
                    if parent and parent.name in ['td', 'th']:
                        next_elem = parent.find_next_sibling()
                        if next_elem:
                            hours = next_elem.get_text(strip=True)
                            # æ”¹è¡Œã‚’æ•´ç†
                            hours = re.sub(r'\s+', ' ', hours)
                            if hours:
                                return hours
            else:
                elem = soup.select_one(selector)
                if elem:
                    hours = elem.get_text(strip=True)
                    hours = re.sub(r'\s+', ' ', hours)
                    if hours:
                        return hours
        
        return ""
    
    def _extract_phone_from_map(self, soup: BeautifulSoup) -> str:
        """åœ°å›³ãƒšãƒ¼ã‚¸ã‹ã‚‰é›»è©±ç•ªå·ã‚’æŠ½å‡º"""
        phone_patterns = [
            r'03-\d{4}-\d{4}',
            r'0\d{2,3}-\d{3,4}-\d{4}',
            r'0\d{9,10}'
        ]
        
        text = soup.get_text()
        for pattern in phone_patterns:
            match = re.search(pattern, text)
            if match:
                return match.group()
        
        return ""
    
    def _extract_address_from_map(self, soup: BeautifulSoup) -> str:
        """åœ°å›³ãƒšãƒ¼ã‚¸ã‹ã‚‰ä½æ‰€ã‚’æŠ½å‡º"""
        # scriptã‚¿ã‚°å†…ã®JSONãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ä½æ‰€ã‚’æ¢ã™
        scripts = soup.find_all('script', type='application/ld+json')
        for script in scripts:
            try:
                data = json.loads(script.string)
                if isinstance(data, dict) and 'address' in data:
                    if isinstance(data['address'], dict):
                        address_parts = []
                        for key in ['addressRegion', 'addressLocality', 'streetAddress']:
                            if key in data['address']:
                                address_parts.append(data['address'][key])
                        if address_parts:
                            return ''.join(address_parts)
                    elif isinstance(data['address'], str):
                        return data['address']
            except:
                pass
        
        return ""
    
    async def scrape_restaurants_batch(self, areas: List[str], max_per_area: int = 100) -> List[Dict]:
        """
        è¤‡æ•°åœ°åŸŸã®ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³æƒ…å ±ã‚’ãƒãƒƒãƒã§å–å¾—
        """
        area_urls = self.get_area_urls()
        all_restaurants = []
        
        for area in areas:
            if area not in area_urls:
                logger.warning(f"æœªå¯¾å¿œã®åœ°åŸŸ: {area}")
                continue
            
            logger.info(f"ğŸ½ï¸ {area}ã®ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹")
            area_url = area_urls[area]
            
            # å¿…è¦ãªãƒšãƒ¼ã‚¸æ•°ã‚’è¨ˆç®—ï¼ˆ1ãƒšãƒ¼ã‚¸20ä»¶ã¨ã—ã¦ï¼‰
            max_pages = (max_per_area + 19) // 20
            
            # åº—èˆ—URLãƒªã‚¹ãƒˆã‚’å–å¾—
            restaurant_urls = await self.scrape_restaurant_list_async(area_url, max_pages)
            
            # æŒ‡å®šä»¶æ•°ã«åˆ¶é™
            restaurant_urls = restaurant_urls[:max_per_area]
            
            # ãƒãƒƒãƒã§è©³ç´°æƒ…å ±ã‚’å–å¾—
            batch_size = 20  # 20ä»¶ãšã¤å‡¦ç†
            for i in range(0, len(restaurant_urls), batch_size):
                batch_urls = restaurant_urls[i:i + batch_size]
                
                tasks = []
                for url in batch_urls:
                    if url not in self.processed_urls:
                        task = self.scrape_restaurant_detail_async(url)
                        tasks.append(task)
                
                if tasks:
                    logger.info(f"ãƒãƒƒãƒå‡¦ç†ä¸­: {i+1}-{min(i+batch_size, len(restaurant_urls))}/{len(restaurant_urls)}")
                    batch_results = await asyncio.gather(*tasks)
                    
                    # æœ‰åŠ¹ãªçµæœã®ã¿è¿½åŠ 
                    valid_results = [r for r in batch_results if r is not None]
                    all_restaurants.extend(valid_results)
                    
                    # é€²æ—ã‚’ä¿å­˜
                    self._save_progress()
        
        # æœ€çµ‚ä¿å­˜
        self._save_progress()
        
        return all_restaurants