#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ã•ã‚ŒãŸéåŒæœŸé£Ÿã¹ãƒ­ã‚°ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼
"""

import asyncio
import aiohttp
from aiohttp import ClientTimeout, TCPConnector
from bs4 import BeautifulSoup
import random
from typing import List, Dict, Optional
from datetime import datetime
from pathlib import Path

from scrapers.base import BaseTabelogScraper
from utils.progress import BatchProgressTracker
from utils.error_handler import (
    ErrorHandler, NetworkError, ParseError,
    retry_on_error, NetworkErrorHandler
)
from utils.validators import RestaurantData, DataValidator
from config.settings import settings
from config.constants import USER_AGENTS


class TabelogAsyncScraper(BaseTabelogScraper):
    """éåŒæœŸé£Ÿã¹ãƒ­ã‚°ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
    
    def __init__(self, max_concurrent: Optional[int] = None):
        """
        åˆæœŸåŒ–
        
        Args:
            max_concurrent: æœ€å¤§åŒæ™‚æ¥ç¶šæ•°
        """
        super().__init__()
        
        # è¨­å®šã‚’èª­ã¿è¾¼ã¿
        self.max_concurrent = max_concurrent or settings.max_concurrent
        self.delay_range = settings.get_delay_range()
        self.timeout = settings.timeout
        self.batch_size = settings.batch_size
        
        # ã‚»ãƒãƒ•ã‚©ã¨ã‚»ãƒƒã‚·ãƒ§ãƒ³
        self.semaphore = asyncio.Semaphore(self.max_concurrent)
        self.session: Optional[aiohttp.ClientSession] = None
        
        # é€²æ—ãƒˆãƒ©ãƒƒã‚«ãƒ¼
        progress_file = Path(settings.cache_dir) / 'async_progress.json'
        results_file = Path(settings.cache_dir) / 'async_results.json'
        self.progress_tracker = BatchProgressTracker(
            str(progress_file),
            str(results_file),
            self.batch_size
        )
        
        # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
        self.error_handler = ErrorHandler(max_retries=3)
    
    async def __aenter__(self):
        """éåŒæœŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®é–‹å§‹"""
        timeout = ClientTimeout(total=self.timeout, connect=10, sock_read=10)
        connector = TCPConnector(limit=self.max_concurrent, force_close=True)
        self.session = aiohttp.ClientSession(
            timeout=timeout,
            connector=connector,
            headers={
                'User-Agent': USER_AGENTS['windows'],
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1'
            }
        )
        
        # é€²æ—ã‚’èª­ã¿è¾¼ã‚€
        self.progress_tracker.load_progress()
        
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """éåŒæœŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®çµ‚äº†"""
        if self.session:
            await self.session.close()
    
    async def scrape_restaurants(
        self,
        areas: List[str],
        max_per_area: int
    ) -> List[Dict]:
        """
        ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³æƒ…å ±ã‚’éåŒæœŸã§å–å¾—
        
        Args:
            areas: å¯¾è±¡åœ°åŸŸãƒªã‚¹ãƒˆ
            max_per_area: åœ°åŸŸã‚ãŸã‚Šã®æœ€å¤§å–å¾—ä»¶æ•°
            
        Returns:
            ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³æƒ…å ±ã®ãƒªã‚¹ãƒˆ
        """
        all_restaurants = []
        area_urls = self.get_area_urls()
        
        for area in areas:
            if area not in area_urls:
                self.log_warning(f"æœªå¯¾å¿œã®åœ°åŸŸ: {area}")
                continue
            
            self.log_info(f"ğŸ½ï¸ {area}ã®ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹")
            area_url = area_urls[area]
            
            # å¿…è¦ãªãƒšãƒ¼ã‚¸æ•°ã‚’è¨ˆç®—ï¼ˆ1ãƒšãƒ¼ã‚¸20ä»¶ã¨ã—ã¦ï¼‰
            max_pages = (max_per_area + 19) // 20
            
            # åº—èˆ—URLãƒªã‚¹ãƒˆã‚’å–å¾—
            restaurant_urls = await self._scrape_restaurant_list(area_url, max_pages)
            
            # æŒ‡å®šä»¶æ•°ã«åˆ¶é™
            restaurant_urls = restaurant_urls[:max_per_area]
            self.progress_tracker.set_total_items(len(restaurant_urls))
            
            # ãƒãƒƒãƒã§è©³ç´°æƒ…å ±ã‚’å–å¾—
            restaurants = await self._scrape_details_batch(restaurant_urls)
            all_restaurants.extend(restaurants)
            
            self.log_info(f"âœ… {area}ã‹ã‚‰ {len(restaurants)} ä»¶å–å¾—å®Œäº†")
        
        # æœ€çµ‚ä¿å­˜
        self.progress_tracker.save_progress()
        
        return all_restaurants
    
    @retry_on_error(exceptions=(NetworkError,), max_attempts=3)
    async def _fetch_page(self, url: str) -> Optional[str]:
        """ãƒšãƒ¼ã‚¸ã‚’éåŒæœŸã§å–å¾—"""
        async with self.semaphore:
            try:
                # ãƒ©ãƒ³ãƒ€ãƒ ãªé…å»¶
                delay = random.uniform(*self.delay_range)
                await asyncio.sleep(delay)
                
                async with self.session.get(url) as response:
                    if response.status == 200:
                        return await response.text()
                    elif response.status == 429:
                        raise NetworkError(f"Rate limited: {url}")
                    else:
                        self.log_warning(f"HTTPã‚¨ãƒ©ãƒ¼ {response.status}: {url}")
                        return None
                        
            except asyncio.TimeoutError:
                self.error_handler.log_error_with_context(
                    TimeoutError("Request timed out"),
                    "ãƒšãƒ¼ã‚¸å–å¾—",
                    url
                )
                raise NetworkError(f"Timeout: {url}")
            except Exception as e:
                self.error_handler.log_error_with_context(e, "ãƒšãƒ¼ã‚¸å–å¾—", url)
                raise
    
    async def _scrape_restaurant_list(
        self,
        area_url: str,
        max_pages: int
    ) -> List[str]:
        """ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³ãƒªã‚¹ãƒˆãƒšãƒ¼ã‚¸ã‹ã‚‰å€‹åˆ¥åº—èˆ—URLã‚’å–å¾—"""
        restaurant_urls = []
        
        # ãƒšãƒ¼ã‚¸URLã‚’ç”Ÿæˆ
        page_urls = [area_url]
        for page in range(2, max_pages + 1):
            page_urls.append(f"{area_url}rstLst/{page}/")
        
        # éåŒæœŸã§ãƒšãƒ¼ã‚¸ã‚’å–å¾—
        tasks = [self._fetch_page(url) for url in page_urls]
        pages = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å„ãƒšãƒ¼ã‚¸ã‹ã‚‰åº—èˆ—URLã‚’æŠ½å‡º
        for i, result in enumerate(pages):
            if isinstance(result, Exception):
                self.log_error(f"ãƒšãƒ¼ã‚¸ {i+1} ã®å–å¾—ã«å¤±æ•—: {result}")
                continue
                
            if result:
                try:
                    soup = BeautifulSoup(result, 'html.parser')
                    links = self._extract_restaurant_links(soup)
                    
                    for href in links:
                        full_url = f"{self.base_url}{href}" if href.startswith('/') else href
                        if full_url not in restaurant_urls:
                            restaurant_urls.append(full_url)
                    
                    self.log_info(f"ãƒšãƒ¼ã‚¸ {i+1}: {len(links)}ä»¶ã®åº—èˆ—URLå–å¾—")
                except Exception as e:
                    self.error_handler.log_error_with_context(
                        e, f"ãƒšãƒ¼ã‚¸ {i+1} ã®ãƒ‘ãƒ¼ã‚¹", page_urls[i]
                    )
        
        self.log_info(f"åˆè¨ˆ {len(restaurant_urls)} ä»¶ã®åº—èˆ—URLå–å¾—")
        return restaurant_urls
    
    async def _scrape_details_batch(self, restaurant_urls: List[str]) -> List[Dict]:
        """ãƒãƒƒãƒã§è©³ç´°æƒ…å ±ã‚’å–å¾—"""
        all_restaurants = []
        
        for i in range(0, len(restaurant_urls), self.batch_size):
            batch_urls = restaurant_urls[i:i + self.batch_size]
            batch_end = min(i + self.batch_size, len(restaurant_urls))
            
            # æœªå‡¦ç†ã®URLã®ã¿å–å¾—
            urls_to_process = [
                url for url in batch_urls 
                if not self.progress_tracker.is_processed(url)
            ]
            
            if not urls_to_process:
                continue
            
            self.progress_tracker.start_batch(i // self.batch_size + 1, i, batch_end)
            
            # è©³ç´°æƒ…å ±ã‚’éåŒæœŸã§å–å¾—
            tasks = [self._scrape_restaurant_detail(url) for url in urls_to_process]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # æœ‰åŠ¹ãªçµæœã®ã¿å‡¦ç†
            for url, result in zip(urls_to_process, results):
                if isinstance(result, Exception):
                    self.log_error(f"è©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼ {url}: {result}")
                    continue
                
                if result:
                    all_restaurants.append(result)
                    self.progress_tracker.add_result(result)
                
                self.progress_tracker.mark_as_processed(url)
            
            # ãƒãƒƒãƒå®Œäº†
            self.progress_tracker.complete_batch()
        
        return all_restaurants
    
    async def _scrape_restaurant_detail(self, restaurant_url: str) -> Optional[Dict]:
        """å€‹åˆ¥åº—èˆ—ãƒšãƒ¼ã‚¸ã‹ã‚‰è©³ç´°æƒ…å ±ã‚’å–å¾—"""
        try:
            # ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã‚’å–å¾—
            main_html = await self._fetch_page(restaurant_url)
            if not main_html:
                return None
            
            soup = BeautifulSoup(main_html, 'html.parser')
            
            # åŸºæœ¬æƒ…å ±ã‚’æŠ½å‡º
            shop_name = self._extract_shop_name(soup)
            if not shop_name:
                self.log_warning(f"åº—åãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ: {restaurant_url}")
                return None
            
            # è©³ç´°æƒ…å ±ã‚’æŠ½å‡º
            restaurant_data = RestaurantData(
                shop_name=shop_name,
                phone=self._extract_phone(soup),
                address=self._extract_address(soup),
                genre=self._extract_genre(soup),
                station=self._extract_station(soup),
                open_time=self._extract_open_time(soup),
                url=restaurant_url,
                source='é£Ÿã¹ãƒ­ã‚°'
            )
            
            # ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
            is_valid, errors = DataValidator.validate_restaurant_data(restaurant_data)
            if not is_valid:
                self.log_warning(f"ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã‚¨ãƒ©ãƒ¼ {restaurant_url}: {errors}")
                return None
            
            # é›»è©±ç•ªå·ã¨ä½æ‰€ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã¯åœ°å›³ãƒšãƒ¼ã‚¸ã‚‚ç¢ºèª
            if not restaurant_data.phone or not restaurant_data.address:
                map_url = restaurant_url.rstrip('/') + '/dtlmap/'
                map_html = await self._fetch_page(map_url)
                
                if map_html:
                    map_soup = BeautifulSoup(map_html, 'html.parser')
                    if not restaurant_data.phone:
                        restaurant_data.phone = self._extract_phone(map_soup)
                    if not restaurant_data.address:
                        restaurant_data.address = self._extract_address(map_soup)
            
            # æ­£è¦åŒ–
            restaurant_data.phone = DataValidator.normalize_phone_number(restaurant_data.phone)
            restaurant_data.shop_name = DataValidator.clean_text(restaurant_data.shop_name)
            restaurant_data.address = DataValidator.clean_text(restaurant_data.address)
            
            result_count = len(self.progress_tracker.results) + 1
            self.log_info(f"âœ… åº—èˆ—æƒ…å ±å–å¾—æˆåŠŸ [{result_count}ä»¶ç›®]: {shop_name}")
            
            return restaurant_data.to_dict()
            
        except Exception as e:
            self.error_handler.log_error_with_context(
                e, "åº—èˆ—è©³ç´°å–å¾—", restaurant_url
            )
            return None