#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
éåŒæœŸç‰ˆé£Ÿã¹ãƒ­ã‚°ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
å¤§é‡ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¯¾å¿œã—ãŸé«˜é€Ÿã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼
"""

import asyncio
import aiohttp
from aiohttp import ClientTimeout, TCPConnector
from bs4 import BeautifulSoup
import time
import re
from typing import List, Dict, Optional, Set
import logging
from urllib.parse import urljoin, urlparse
import random
from datetime import datetime
import json
import os
from pathlib import Path

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TabelogScraperAsync:
    """éåŒæœŸç‰ˆé£Ÿã¹ãƒ­ã‚°ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼"""
    
    def __init__(self, max_concurrent: int = 10, delay_range: tuple = (0.5, 1.0)):
        """
        åˆæœŸåŒ–
        
        Args:
            max_concurrent: æœ€å¤§åŒæ™‚æ¥ç¶šæ•°
            delay_range: ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã®é…å»¶ç¯„å›²ï¼ˆç§’ï¼‰
        """
        self.max_concurrent = max_concurrent
        self.delay_range = delay_range
        self.base_url = "https://tabelog.com"
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.session = None
        self.progress_file = Path("cache/scraping_progress.json")
        self.results_file = Path("cache/partial_results.json")
        self.processed_urls: Set[str] = set()
        self.results: List[Dict] = []
        
    async def __aenter__(self):
        """éåŒæœŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®é–‹å§‹"""
        timeout = ClientTimeout(total=30, connect=10, sock_read=10)
        connector = TCPConnector(limit=self.max_concurrent, force_close=True)
        self.session = aiohttp.ClientSession(
            timeout=timeout,
            connector=connector,
            headers={
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
            }
        )
        # é€²æ—ã‚’èª­ã¿è¾¼ã‚€
        self._load_progress()
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """éåŒæœŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®çµ‚äº†"""
        if self.session:
            await self.session.close()
            
    def _load_progress(self):
        """å‰å›ã®é€²æ—ã‚’èª­ã¿è¾¼ã‚€"""
        if self.progress_file.exists():
            try:
                with open(self.progress_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.processed_urls = set(data.get('processed_urls', []))
                    logger.info(f"å‰å›ã®é€²æ—ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {len(self.processed_urls)}ä»¶å‡¦ç†æ¸ˆã¿")
            except Exception as e:
                logger.error(f"é€²æ—èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
                
        if self.results_file.exists():
            try:
                with open(self.results_file, 'r', encoding='utf-8') as f:
                    self.results = json.load(f)
                    logger.info(f"å‰å›ã®çµæœã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {len(self.results)}ä»¶")
            except Exception as e:
                logger.error(f"çµæœèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _save_progress(self):
        """é€²æ—ã‚’ä¿å­˜"""
        try:
            self.progress_file.parent.mkdir(exist_ok=True)
            with open(self.progress_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'processed_urls': list(self.processed_urls),
                    'timestamp': datetime.now().isoformat()
                }, f, ensure_ascii=False, indent=2)
                
            with open(self.results_file, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            logger.error(f"é€²æ—ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def get_area_urls(self) -> Dict[str, str]:
        """åœ°åŸŸåˆ¥URLã‚’å–å¾—"""
        return {
            'æ±äº¬éƒ½': 'https://tabelog.com/tokyo/',
            'å¤§é˜ªåºœ': 'https://tabelog.com/osaka/',
            'ç¥å¥ˆå·çœŒ': 'https://tabelog.com/kanagawa/',
            'æ„›çŸ¥çœŒ': 'https://tabelog.com/aichi/',
            'ç¦å²¡çœŒ': 'https://tabelog.com/fukuoka/',
            'åŒ—æµ·é“': 'https://tabelog.com/hokkaido/',
            'äº¬éƒ½åºœ': 'https://tabelog.com/kyoto/',
            'å…µåº«çœŒ': 'https://tabelog.com/hyogo/',
            'åŸ¼ç‰çœŒ': 'https://tabelog.com/saitama/',
            'åƒè‘‰çœŒ': 'https://tabelog.com/chiba/'
        }
    
    async def fetch_page(self, url: str) -> Optional[str]:
        """ãƒšãƒ¼ã‚¸ã‚’éåŒæœŸã§å–å¾—"""
        async with self.semaphore:
            try:
                # ãƒ©ãƒ³ãƒ€ãƒ ãªé…å»¶
                delay = random.uniform(*self.delay_range)
                await asyncio.sleep(delay)
                
                async with self.session.get(url) as response:
                    if response.status == 200:
                        return await response.text()
                    else:
                        logger.warning(f"HTTPã‚¨ãƒ©ãƒ¼ {response.status}: {url}")
                        return None
                        
            except asyncio.TimeoutError:
                logger.error(f"ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {url}")
                return None
            except Exception as e:
                logger.error(f"å–å¾—ã‚¨ãƒ©ãƒ¼ {url}: {e}")
                return None
    
    async def scrape_restaurant_list_async(self, area_url: str, max_pages: int = 5) -> List[str]:
        """
        ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³ãƒªã‚¹ãƒˆãƒšãƒ¼ã‚¸ã‹ã‚‰å€‹åˆ¥åº—èˆ—URLã‚’éåŒæœŸã§å–å¾—
        """
        restaurant_urls = []
        tasks = []
        
        # ãƒšãƒ¼ã‚¸URLã‚’ç”Ÿæˆ
        page_urls = [area_url]
        for page in range(2, max_pages + 1):
            page_urls.append(f"{area_url}rstLst/{page}/")
        
        # éåŒæœŸã§ãƒšãƒ¼ã‚¸ã‚’å–å¾—
        for page_url in page_urls:
            task = self.fetch_page(page_url)
            tasks.append(task)
        
        pages = await asyncio.gather(*tasks)
        
        # å„ãƒšãƒ¼ã‚¸ã‹ã‚‰åº—èˆ—URLã‚’æŠ½å‡º
        for i, html in enumerate(pages):
            if html:
                soup = BeautifulSoup(html, 'html.parser')
                links = self._extract_restaurant_links(soup)
                
                for href in links:
                    full_url = urljoin(self.base_url, href)
                    if full_url not in restaurant_urls:
                        restaurant_urls.append(full_url)
                
                logger.info(f"ãƒšãƒ¼ã‚¸ {i+1}: {len(links)}ä»¶ã®åº—èˆ—URLå–å¾—")
        
        logger.info(f"åˆè¨ˆ {len(restaurant_urls)} ä»¶ã®åº—èˆ—URLå–å¾—")
        return restaurant_urls
    
    def _extract_restaurant_links(self, soup: BeautifulSoup) -> List[str]:
        """ãƒšãƒ¼ã‚¸ã‹ã‚‰åº—èˆ—ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º"""
        links = []
        
        # è¤‡æ•°ã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦ã™
        selectors = [
            'a.list-rst__rst-name-target',
            'a[href*="/A"][href*="/A"][href*="/"]',
            '.list-rst__rst-name a',
            '.rst-name a'
        ]
        
        for selector in selectors:
            elements = soup.select(selector)
            if elements:
                links = [elem.get('href') for elem in elements if elem.get('href')]
                break
        
        # hrefãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã‚‚æ¤œç´¢
        if not links:
            all_links = soup.find_all('a', href=True)
            links = [link.get('href') for link in all_links 
                    if re.match(r'/[^/]+/A\d+/A\d+/\d+/', link.get('href', ''))]
        
        return links
    
    async def scrape_restaurant_detail_async(self, restaurant_url: str) -> Optional[Dict]:
        """å€‹åˆ¥åº—èˆ—ãƒšãƒ¼ã‚¸ã‹ã‚‰è©³ç´°æƒ…å ±ã‚’éåŒæœŸã§å–å¾—"""
        
        # æ—¢ã«å‡¦ç†æ¸ˆã¿ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        if restaurant_url in self.processed_urls:
            return None
            
        try:
            # ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã¨åœ°å›³ãƒšãƒ¼ã‚¸ã‚’åŒæ™‚ã«å–å¾—
            main_task = self.fetch_page(restaurant_url)
            map_url = restaurant_url.rstrip('/') + '/dtlmap/'
            map_task = self.fetch_page(map_url)
            
            main_html, map_html = await asyncio.gather(main_task, map_task)
            
            if not main_html:
                return None
            
            soup = BeautifulSoup(main_html, 'html.parser')
            
            # åº—åå–å¾—
            shop_name = self._extract_shop_name(soup)
            if not shop_name:
                logger.warning(f"åº—åãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ: {restaurant_url}")
                return None
            
            # é›»è©±ç•ªå·ã¨ä½æ‰€ã‚’åœ°å›³ãƒšãƒ¼ã‚¸ã‹ã‚‰å–å¾—
            phone = ""
            address = ""
            if map_html:
                map_soup = BeautifulSoup(map_html, 'html.parser')
                phone, address = self._extract_contact_info(map_soup)
            
            # ãã®ä»–ã®æƒ…å ±
            genre = self._extract_info(soup, ['.rst-info__category', '.genre'])
            station = self._extract_info(soup, ['.rst-info__station', '.station'])
            open_time = self._extract_info(soup, ['.rst-info__open-hours', '.open-hours'])
            
            restaurant_info = {
                'shop_name': shop_name,
                'phone': phone,
                'address': address,
                'genre': genre,
                'station': station,
                'open_time': open_time,
                'url': restaurant_url,
                'source': 'é£Ÿã¹ãƒ­ã‚°',
                'scraped_at': datetime.now().isoformat()
            }
            
            # å‡¦ç†æ¸ˆã¿ã¨ã—ã¦è¨˜éŒ²
            self.processed_urls.add(restaurant_url)
            self.results.append(restaurant_info)
            
            # å®šæœŸçš„ã«é€²æ—ã‚’ä¿å­˜ï¼ˆ10ä»¶ã”ã¨ï¼‰
            if len(self.results) % 10 == 0:
                self._save_progress()
                logger.info(f"é€²æ—ä¿å­˜: {len(self.results)}ä»¶å®Œäº†")
            
            logger.info(f"âœ… åº—èˆ—æƒ…å ±å–å¾—æˆåŠŸ [{len(self.results)}ä»¶ç›®]: {shop_name}")
            return restaurant_info
            
        except Exception as e:
            logger.error(f"åº—èˆ—è©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼ {restaurant_url}: {e}")
            return None
    
    def _extract_shop_name(self, soup: BeautifulSoup) -> str:
        """åº—åã‚’æŠ½å‡º"""
        selectors = ['h1.display-name', '.rst-name', 'h1', '.shop-name']
        
        for selector in selectors:
            elem = soup.select_one(selector)
            if elem:
                text = elem.get_text(strip=True)
                if ' - ' in text:
                    text = text.split(' - ')[0]
                if '(' in text:
                    text = text.split('(')[0]
                return text.strip()
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã‚¿ã‚°ã‹ã‚‰å–å¾—
        title = soup.select_one('title')
        if title:
            text = title.get_text(strip=True)
            if ' - ' in text:
                return text.split(' - ')[0].strip()
        
        return ""
    
    def _extract_contact_info(self, soup: BeautifulSoup) -> tuple:
        """é›»è©±ç•ªå·ã¨ä½æ‰€ã‚’æŠ½å‡º"""
        phone = ""
        address = ""
        
        # é›»è©±ç•ªå·
        phone_patterns = [
            r'03-\d{4}-\d{4}',
            r'0\d{2,3}-\d{3,4}-\d{4}',
            r'0\d{9,10}'
        ]
        
        text = soup.get_text()
        for pattern in phone_patterns:
            match = re.search(pattern, text)
            if match:
                phone = match.group()
                break
        
        # ä½æ‰€
        address_selectors = ['.rst-info__address', '.address', 'p:contains("ä½æ‰€")']
        for selector in address_selectors:
            elem = soup.select_one(selector)
            if elem:
                address = elem.get_text(strip=True)
                address = re.sub(r'^ä½æ‰€[ï¼š:]', '', address).strip()
                break
        
        return phone, address
    
    def _extract_info(self, soup: BeautifulSoup, selectors: List[str]) -> str:
        """æ±ç”¨æƒ…å ±æŠ½å‡º"""
        for selector in selectors:
            elem = soup.select_one(selector)
            if elem:
                return elem.get_text(strip=True)
        return ""
    
    async def scrape_restaurants_batch(self, areas: List[str], max_per_area: int = 100) -> List[Dict]:
        """
        è¤‡æ•°åœ°åŸŸã®ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³æƒ…å ±ã‚’ãƒãƒƒãƒã§å–å¾—
        """
        area_urls = self.get_area_urls()
        all_restaurants = []
        
        for area in areas:
            if area not in area_urls:
                logger.warning(f"æœªå¯¾å¿œã®åœ°åŸŸ: {area}")
                continue
            
            logger.info(f"ğŸ½ï¸ {area}ã®ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹")
            area_url = area_urls[area]
            
            # å¿…è¦ãªãƒšãƒ¼ã‚¸æ•°ã‚’è¨ˆç®—ï¼ˆ1ãƒšãƒ¼ã‚¸20ä»¶ã¨ã—ã¦ï¼‰
            max_pages = (max_per_area + 19) // 20
            
            # åº—èˆ—URLãƒªã‚¹ãƒˆã‚’å–å¾—
            restaurant_urls = await self.scrape_restaurant_list_async(area_url, max_pages)
            
            # æŒ‡å®šä»¶æ•°ã«åˆ¶é™
            restaurant_urls = restaurant_urls[:max_per_area]
            
            # ãƒãƒƒãƒã§è©³ç´°æƒ…å ±ã‚’å–å¾—
            batch_size = 20  # 20ä»¶ãšã¤å‡¦ç†
            for i in range(0, len(restaurant_urls), batch_size):
                batch_urls = restaurant_urls[i:i + batch_size]
                
                tasks = []
                for url in batch_urls:
                    if url not in self.processed_urls:
                        task = self.scrape_restaurant_detail_async(url)
                        tasks.append(task)
                
                if tasks:
                    logger.info(f"ãƒãƒƒãƒå‡¦ç†ä¸­: {i+1}-{min(i+batch_size, len(restaurant_urls))}/{len(restaurant_urls)}")
                    batch_results = await asyncio.gather(*tasks)
                    
                    # æœ‰åŠ¹ãªçµæœã®ã¿è¿½åŠ 
                    valid_results = [r for r in batch_results if r is not None]
                    all_restaurants.extend(valid_results)
                    
                    # é€²æ—ã‚’ä¿å­˜
                    self._save_progress()
        
        # æœ€çµ‚ä¿å­˜
        self._save_progress()
        
        return all_restaurants